"use strict";(globalThis.webpackChunkteaching_physical_ai_robotics_book=globalThis.webpackChunkteaching_physical_ai_robotics_book||[]).push([[303],{8390:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-04/module-isaac","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac Platform)","description":"\ud83d\udcda Learning Path Overview","source":"@site/docs/chapter-04/module-isaac.md","sourceDirName":"chapter-04","slug":"/chapter-04/module-isaac","permalink":"/humanoid-robotics-book/docs/chapter-04/module-isaac","draft":false,"unlisted":false,"editUrl":"https://github.com/Aisha-Sarfaraz/humanoid-robotics-book/tree/main/docs/chapter-04/module-isaac.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 2: The Digital Twin (Gazebo & Unity Simulation)","permalink":"/humanoid-robotics-book/docs/chapter-04/module-simulation"},"next":{"title":"Module 4: The Cognitive Layer - Vision-Language-Action (VLA) Models","permalink":"/humanoid-robotics-book/docs/chapter-04/module-vla"}}');var r=i(4848),t=i(8453);const a={},o="Module 3: The AI-Robot Brain (NVIDIA Isaac Platform)",l={},c=[{value:"\ud83d\udcda Learning Path Overview",id:"-learning-path-overview",level:2},{value:"Week 1: Isaac Sim - GPU-Accelerated Photorealistic Simulation",id:"week-1-isaac-sim---gpu-accelerated-photorealistic-simulation",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory",level:3},{value:"\ud83d\udcbb Code Example 1: Isaac Sim Python API - Spawn Robot",id:"-code-example-1-isaac-sim-python-api---spawn-robot",level:3},{value:"\ud83d\udcbb Code Example 2: Isaac ROS Integration",id:"-code-example-2-isaac-ros-integration",level:3},{value:"\ud83d\udd2c Lab Exercise 1: Warehouse Environment",id:"-lab-exercise-1-warehouse-environment",level:3},{value:"\u2705 Week 1 Assessment Checklist",id:"-week-1-assessment-checklist",level:3},{value:"Week 2: Perception Pipeline - Object Detection &amp; Segmentation",id:"week-2-perception-pipeline---object-detection--segmentation",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-1",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-1",level:3},{value:"\ud83d\udcbb Code Example 3: Synthetic Data Generation",id:"-code-example-3-synthetic-data-generation",level:3},{value:"\ud83d\udcbb Code Example 4: Deploy Perception with Isaac ROS",id:"-code-example-4-deploy-perception-with-isaac-ros",level:3},{value:"\ud83d\udd2c Lab Exercise 2: Train Object Detector on Synthetic Data",id:"-lab-exercise-2-train-object-detector-on-synthetic-data",level:3},{value:"\u2705 Week 2 Assessment Checklist",id:"-week-2-assessment-checklist",level:3},{value:"Week 3: Deployment &amp; Sim-to-Real Transfer",id:"week-3-deployment--sim-to-real-transfer",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-2",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-2",level:3},{value:"\ud83d\udd2c Lab Exercise 3: Full Navigation Pipeline",id:"-lab-exercise-3-full-navigation-pipeline",level:3},{value:"\u2705 Week 3 Assessment Checklist",id:"-week-3-assessment-checklist",level:3},{value:"\ud83d\udcdd Module Summary",id:"-module-summary",level:2},{value:"Key Concepts Mastered",id:"key-concepts-mastered",level:3},{value:"Real-World Applications",id:"real-world-applications",level:3},{value:"Common Pitfalls &amp; Solutions",id:"common-pitfalls--solutions",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"\ud83c\udf93 Final Project: Sim-to-Real Object Manipulation",id:"-final-project-sim-to-real-object-manipulation",level:2},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-3-the-ai-robot-brain-nvidia-isaac-platform",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac Platform)"})}),"\n",(0,r.jsx)(n.h2,{id:"-learning-path-overview",children:"\ud83d\udcda Learning Path Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration:"})," 3 weeks (12-16 hours/week)\n",(0,r.jsx)(n.strong,{children:"Difficulty:"})," Advanced\n",(0,r.jsx)(n.strong,{children:"Prerequisites:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Completed Modules 1 & 2 (ROS 2 + Simulation)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Python programming with NumPy"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Basic machine learning concepts"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 GPU/CUDA awareness (helpful but not required)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning Progression:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Week 1: Isaac Sim Setup] --\x3e B[Week 2: Perception Pipeline]\n    B --\x3e C[Week 3: Synthetic Data & Training]\n    C --\x3e D[Deploy to Hardware]\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"By the end of this module:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Use NVIDIA Isaac Sim for photorealistic GPU-accelerated simulation"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Implement perception pipelines (object detection, segmentation, depth)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Generate massive synthetic datasets for ML training"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Deploy trained models on real robots with Isaac ROS"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-1-isaac-sim---gpu-accelerated-photorealistic-simulation",children:"Week 1: Isaac Sim - GPU-Accelerated Photorealistic Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up NVIDIA Isaac Sim and understand Omniverse architecture"}),"\n",(0,r.jsx)(n.li,{children:"Create photorealistic environments with RTX ray tracing"}),"\n",(0,r.jsx)(n.li,{children:"Simulate robots with physics (PhysX 5) at real-time speeds"}),"\n",(0,r.jsx)(n.li,{children:"Interface Isaac Sim with ROS 2"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why Isaac Sim?"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Feature      \u2502  Gazebo  \u2502  Unity   \u2502 Isaac Sim  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Graphics        \u2502 Moderate \u2502 High     \u2502 Photorealistic (RTX) \u2502\n\u2502 Physics         \u2502 Accurate \u2502 Good     \u2502 PhysX 5 (GPU) \u2502\n\u2502 Performance     \u2502 CPU      \u2502 GPU      \u2502 GPU (10-100x faster) \u2502\n\u2502 ML Integration  \u2502 Limited  \u2502 Good     \u2502 Excellent  \u2502\n\u2502 Synthetic Data  \u2502 Basic    \u2502 Good     \u2502 Domain Randomization \u2502\n\u2502 ROS 2 Support   \u2502 Native   \u2502 Bridge   \u2502 Isaac ROS  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Isaac Sim Architecture:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Omniverse Kit:"})," Core USD (Universal Scene Description) framework"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PhysX 5:"})," GPU-accelerated physics engine"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX Ray Tracing:"})," Photorealistic rendering with ray-traced lighting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac Gym:"})," RL training with thousands of parallel simulations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS:"})," Hardware-accelerated perception for deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-1-isaac-sim-python-api---spawn-robot",children:"\ud83d\udcbb Code Example 1: Isaac Sim Python API - Spawn Robot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: isaac_sim_spawn_robot.py\n# Run this inside Isaac Sim Python environment\n\nfrom omni.isaac.kit import SimulationApp\n\n# Initialize simulation\nsimulation_app = SimulationApp({"headless": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nimport carb\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\n\n# Add ground plane\nworld.scene.add_default_ground_plane()\n\n# Import robot URDF/USD\nrobot_usd_path = "omniverse://localhost/NVIDIA/Assets/Isaac/2023.1.1/Isaac/Robots/Franka/franka.usd"\n\n# Spawn robot at position\nrobot_prim_path = "/World/Franka"\nadd_reference_to_stage(usd_path=robot_usd_path, prim_path=robot_prim_path)\n\n# Create robot interface\nrobot = world.scene.add(\n    Robot(\n        prim_path=robot_prim_path,\n        name="franka_robot",\n        position=[0, 0, 0]\n    )\n)\n\n# Reset world\nworld.reset()\n\nprint("Robot spawned successfully!")\nprint(f"Robot DOF: {robot.num_dof}")\nprint(f"Joint names: {robot.dof_names}")\n\n# Simulation loop\nwhile simulation_app.is_running():\n    # Step physics\n    world.step(render=True)\n\n    # Get robot state\n    if world.is_playing():\n        joint_positions, joint_velocities = robot.get_joint_positions(), robot.get_joint_velocities()\n\n        # Simple control: move joints sinusoidally\n        import numpy as np\n        time = world.current_time_step_index * 0.01\n        target_positions = np.sin(time + np.arange(robot.num_dof)) * 0.3\n\n        robot.set_joint_positions(target_positions)\n\n# Cleanup\nsimulation_app.close()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-2-isaac-ros-integration",children:"\ud83d\udcbb Code Example 2: Isaac ROS Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: isaac_ros_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\n\n# Must run inside Isaac Sim\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.extensions import enable_extension\nenable_extension(\"omni.isaac.ros2_bridge\")\n\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.core.prims import XFormPrim\n\nclass IsaacROSBridge(Node):\n    def __init__(self, world):\n        super().__init__('isaac_ros_bridge')\n\n        self.world = world\n        self.bridge = CvBridge()\n\n        # Create ROS 2 publishers\n        self.image_pub = self.create_publisher(Image, '/isaac/camera/image', 10)\n        self.depth_pub = self.create_publisher(Image, '/isaac/camera/depth', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, '/isaac/camera/info', 10)\n\n        # Subscribe to velocity commands\n        self.cmd_vel_sub = self.create_subscription(\n            Twist,\n            '/cmd_vel',\n            self.cmd_vel_callback,\n            10\n        )\n\n        # Add camera to scene\n        self.camera = world.scene.add(\n            Camera(\n                prim_path=\"/World/Camera\",\n                position=np.array([2.0, 0.0, 1.5]),\n                frequency=30,\n                resolution=(1280, 720)\n            )\n        )\n\n        # Timer for publishing\n        self.create_timer(0.033, self.publish_camera_data)  # 30 Hz\n\n        self.get_logger().info('Isaac ROS Bridge ready')\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Receive velocity commands from ROS\"\"\"\n        linear_x = msg.linear.x\n        angular_z = msg.angular.z\n\n        self.get_logger().info(f'Received cmd_vel: linear={linear_x}, angular={angular_z}')\n\n        # Apply to robot in Isaac Sim\n        # (Implementation depends on robot type - differential drive, omni, etc.)\n\n    def publish_camera_data(self):\n        \"\"\"Publish camera images to ROS topics\"\"\"\n        if not self.world.is_playing():\n            return\n\n        # Get RGB image\n        rgb_data = self.camera.get_rgba()[:, :, :3]  # Drop alpha channel\n\n        # Get depth image\n        depth_data = self.camera.get_depth()\n\n        # Convert to ROS messages\n        rgb_msg = self.bridge.cv2_to_imgmsg(rgb_data, encoding='rgb8')\n        rgb_msg.header.stamp = self.get_clock().now().to_msg()\n        rgb_msg.header.frame_id = 'isaac_camera'\n\n        depth_msg = self.bridge.cv2_to_imgmsg(depth_data, encoding='32FC1')\n        depth_msg.header = rgb_msg.header\n\n        # Publish\n        self.image_pub.publish(rgb_msg)\n        self.depth_pub.publish(depth_msg)\n\n        # Camera info\n        info_msg = CameraInfo()\n        info_msg.header = rgb_msg.header\n        info_msg.width = 1280\n        info_msg.height = 720\n        # Fill in intrinsics...\n        self.camera_info_pub.publish(info_msg)\n\ndef main():\n    # Initialize Isaac Sim\n    from omni.isaac.kit import SimulationApp\n    simulation_app = SimulationApp({\"headless\": False})\n\n    # Initialize ROS 2\n    rclpy.init()\n\n    # Create world\n    world = World()\n    world.scene.add_default_ground_plane()\n    world.reset()\n\n    # Create ROS bridge node\n    bridge_node = IsaacROSBridge(world)\n\n    # Simulation loop\n    import threading\n\n    def ros_spin():\n        rclpy.spin(bridge_node)\n\n    ros_thread = threading.Thread(target=ros_spin, daemon=True)\n    ros_thread.start()\n\n    while simulation_app.is_running():\n        world.step(render=True)\n\n    # Cleanup\n    bridge_node.destroy_node()\n    rclpy.shutdown()\n    simulation_app.close()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-1-warehouse-environment",children:"\ud83d\udd2c Lab Exercise 1: Warehouse Environment"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Build a photorealistic warehouse in Isaac Sim"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Import warehouse USD assets from Omniverse library"}),"\n",(0,r.jsx)(n.li,{children:"Add shelves, boxes, and pallets"}),"\n",(0,r.jsx)(n.li,{children:"Configure RTX lighting (sun, area lights)"}),"\n",(0,r.jsx)(n.li,{children:"Spawn a mobile robot (Carter, Jetbot, or custom)"}),"\n",(0,r.jsx)(n.li,{children:"Stream camera feed to ROS 2"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bonus Challenge:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add conveyor belts with PhysX"}),"\n",(0,r.jsx)(n.li,{children:"Implement random object spawning"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Isaac Sim USD scene file"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 node that receives images"}),"\n",(0,r.jsx)(n.li,{children:"Screenshot/video of photorealistic rendering"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-1-assessment-checklist",children:"\u2705 Week 1 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Install NVIDIA Isaac Sim (Omniverse)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Spawn robot using Python API"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configure camera and lighting"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Publish sensor data to ROS 2 topics"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understand USD scene graph structure"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-2-perception-pipeline---object-detection--segmentation",children:"Week 2: Perception Pipeline - Object Detection & Segmentation"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-1",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Use Isaac Sim synthetic data generation"}),"\n",(0,r.jsx)(n.li,{children:"Implement object detection with NVIDIA TAO/Triton"}),"\n",(0,r.jsx)(n.li,{children:"Perform semantic/instance segmentation"}),"\n",(0,r.jsx)(n.li,{children:"Deploy perception models with Isaac ROS GEMs"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-1",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Synthetic Data for Perception:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Traditional Approach          Isaac Sim Approach\n\u251c\u2500 Collect real images        \u251c\u2500 Generate infinite data\n\u251c\u2500 Manual labeling            \u251c\u2500 Perfect auto-labels\n\u251c\u2500 Limited scenarios          \u251c\u2500 Domain randomization\n\u251c\u2500 Expensive & slow           \u251c\u2500 Free & instant\n\u2514\u2500 Privacy concerns           \u2514\u2500 No privacy issues\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization:"}),"\nVary lighting, textures, object positions, camera angles to train robust models that generalize to real world."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Isaac ROS GEMs (Graph Execution Modules):"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Hardware-accelerated perception nodes"}),"\n",(0,r.jsx)(n.li,{children:"GPU-optimized DNN inference (TensorRT)"}),"\n",(0,r.jsx)(n.li,{children:"Run on Jetson/x86 with CUDA"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-3-synthetic-data-generation",children:"\ud83d\udcbb Code Example 3: Synthetic Data Generation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: generate_synthetic_dataset.py\nfrom omni.isaac.kit import SimulationApp\nsimulation_app = SimulationApp({"headless": True})  # Headless for speed\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.replicator.core import BasicWriter, AnnotatorRegistry\nimport omni.replicator.core as rep\nimport random\n\n# Create world\nworld = World()\nworld.scene.add_default_ground_plane()\n\n# Add camera\ncamera = rep.create.camera(position=(3, 0, 2), look_at=(0, 0, 0.5))\n\n# Define object library\nobject_models = [\n    "omniverse://localhost/NVIDIA/Assets/ArchVis/Commercial/Warehouse/Props/SM_CardboardBox_A_01.usd",\n    "omniverse://localhost/NVIDIA/Assets/ArchVis/Commercial/Warehouse/Props/SM_PalletJack_01.usd",\n    # Add more objects...\n]\n\ndef randomize_scene():\n    """Randomize lighting, objects, and camera for domain randomization"""\n\n    # Random lighting\n    with rep.trigger.on_frame():\n        # Randomize dome light\n        light = rep.create.light(\n            light_type="Dome",\n            intensity=rep.distribution.uniform(500, 2000),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n\n        # Randomize object positions\n        num_objects = random.randint(5, 15)\n        for _ in range(num_objects):\n            obj_model = random.choice(object_models)\n\n            obj = rep.create.from_usd(obj_model)\n\n            with obj:\n                rep.modify.pose(\n                    position=rep.distribution.uniform((-2, -2, 0.1), (2, 2, 1.5)),\n                    rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n                )\n\n                # Random material/texture\n                rep.randomizer.materials(\n                    materials=rep.get.prims(semantics=[("class", "material")])\n                )\n\n        # Random camera angle\n        with camera:\n            rep.modify.pose(\n                position=rep.distribution.uniform((2, -2, 1), (4, 2, 3)),\n                look_at=(0, 0, 0.5)\n            )\n\n# Setup annotations (labels)\nrender_product = rep.create.render_product(camera, (1280, 720))\n\n# Register writers for different annotations\nrgb_writer = rep.WriterRegistry.get("BasicWriter")\nrgb_writer.initialize(\n    output_dir="./synthetic_data/rgb",\n    rgb=True\n)\n\nbbox_writer = rep.WriterRegistry.get("BasicWriter")\nbbox_writer.initialize(\n    output_dir="./synthetic_data/labels",\n    bounding_box_2d_tight=True,\n    semantic_segmentation=True\n)\n\n# Attach to render product\nrgb_writer.attach([render_product])\nbbox_writer.attach([render_product])\n\n# Generate dataset\nNUM_FRAMES = 1000\n\nworld.reset()\n\nwith rep.trigger.on_frame(num_frames=NUM_FRAMES):\n    randomize_scene()\n\n# Run simulation\nprint(f"Generating {NUM_FRAMES} synthetic training images...")\n\nfor frame in range(NUM_FRAMES):\n    world.step(render=True)\n\n    if frame % 100 == 0:\n        print(f"Progress: {frame}/{NUM_FRAMES} frames")\n\nprint("Dataset generation complete!")\nprint("Output: ./synthetic_data/")\n\nsimulation_app.close()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-4-deploy-perception-with-isaac-ros",children:"\ud83d\udcbb Code Example 4: Deploy Perception with Isaac ROS"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: isaac_ros_perception.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom cv_bridge import CvBridge\nimport numpy as np\n\n# Isaac ROS GEM for DNN inference\ntry:\n    from isaac_ros_dnn_inference import TensorRTNode\n    ISAAC_ROS_AVAILABLE = True\nexcept ImportError:\n    ISAAC_ROS_AVAILABLE = False\n    print("WARNING: Isaac ROS not available, using CPU inference")\n\nclass PerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'perception_pipeline\')\n\n        self.bridge = CvBridge()\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/isaac/camera/image\',\n            self.image_callback,\n            10\n        )\n\n        # Publish detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            \'/detections\',\n            10\n        )\n\n        # Load object detection model (YOLOv8, DOPE, etc.)\n        if ISAAC_ROS_AVAILABLE:\n            # Use TensorRT for GPU acceleration\n            self.model = self.load_tensorrt_model()\n        else:\n            # Fallback to ONNX/PyTorch CPU\n            self.model = self.load_cpu_model()\n\n        self.get_logger().info(\'Perception pipeline ready\')\n\n    def load_tensorrt_model(self):\n        """Load TensorRT optimized model for Jetson/CUDA"""\n        # Placeholder - actual implementation uses Isaac ROS GEMs\n        self.get_logger().info(\'Loading TensorRT model for GPU inference\')\n        return None  # Would return TensorRT engine\n\n    def load_cpu_model(self):\n        """Fallback CPU inference"""\n        # Example: load YOLOv8 with ultralytics\n        try:\n            from ultralytics import YOLO\n            model = YOLO(\'yolov8n.pt\')\n            self.get_logger().info(\'Loaded YOLOv8 CPU model\')\n            return model\n        except:\n            self.get_logger().warn(\'Could not load model\')\n            return None\n\n    def image_callback(self, msg):\n        """Process camera image for object detection"""\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n\n        # Run inference\n        detections = self.detect_objects(cv_image)\n\n        # Publish results\n        detection_msg = Detection2DArray()\n        detection_msg.header = msg.header\n        detection_msg.detections = detections\n\n        self.detection_pub.publish(detection_msg)\n\n        self.get_logger().info(f\'Detected {len(detections)} objects\')\n\n    def detect_objects(self, image):\n        """Run object detection inference"""\n        if self.model is None:\n            return []\n\n        # Run model\n        results = self.model(image, conf=0.5)\n\n        # Convert to ROS Detection2D messages\n        detections = []\n\n        for result in results:\n            for box in result.boxes:\n                detection = Detection2D()\n\n                # Bounding box\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                detection.bbox.center.position.x = float((x1 + x2) / 2)\n                detection.bbox.center.position.y = float((y1 + y2) / 2)\n                detection.bbox.size_x = float(x2 - x1)\n                detection.bbox.size_y = float(y2 - y1)\n\n                # Class and confidence\n                # detection.results[0].hypothesis.class_id = str(int(box.cls))\n                # detection.results[0].hypothesis.score = float(box.conf)\n\n                detections.append(detection)\n\n        return detections\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionPipeline()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-2-train-object-detector-on-synthetic-data",children:"\ud83d\udd2c Lab Exercise 2: Train Object Detector on Synthetic Data"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Task:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Generate 5,000 synthetic images in Isaac Sim with domain randomization"}),"\n",(0,r.jsx)(n.li,{children:"Export in COCO or YOLO format"}),"\n",(0,r.jsx)(n.li,{children:"Train YOLOv8 or TAO model"}),"\n",(0,r.jsx)(n.li,{children:"Deploy on Isaac ROS with TensorRT"}),"\n",(0,r.jsx)(n.li,{children:"Test on real camera feed"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Metrics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measure inference FPS (CPU vs GPU)"}),"\n",(0,r.jsx)(n.li,{children:"Calculate mAP on test set"}),"\n",(0,r.jsx)(n.li,{children:"Compare sim-trained vs real-trained"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Trained model weights (.pt or .engine)"}),"\n",(0,r.jsx)(n.li,{children:"Training curves (loss, mAP)"}),"\n",(0,r.jsx)(n.li,{children:"Inference demo video"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-2-assessment-checklist",children:"\u2705 Week 2 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Generate 1000+ synthetic labeled images"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Train object detection model"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Convert model to TensorRT"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Deploy with Isaac ROS GEM"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Measure inference performance (FPS, accuracy)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-3-deployment--sim-to-real-transfer",children:"Week 3: Deployment & Sim-to-Real Transfer"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-2",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Understand sim-to-real gap challenges"}),"\n",(0,r.jsx)(n.li,{children:"Use domain randomization techniques"}),"\n",(0,r.jsx)(n.li,{children:"Deploy perception to NVIDIA Jetson"}),"\n",(0,r.jsx)(n.li,{children:"Integrate with navigation stack"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-2",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sim-to-Real Gap:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Simulation                 Real World\n\u251c\u2500 Perfect sensors         \u251c\u2500 Noisy sensors\n\u251c\u2500 Known lighting          \u251c\u2500 Variable lighting\n\u251c\u2500 Clean textures          \u251c\u2500 Dirt, reflections\n\u251c\u2500 Exact physics           \u251c\u2500 Imperfect physics\n\u2514\u2500 No occlusions           \u2514\u2500 Dynamic occlusions\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bridging the Gap:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization:"})," Vary everything (lighting, textures, noise)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Realism:"})," Add noise models matching real sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Progressive Training:"})," Start sim, fine-tune on small real data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sim2Real Metrics:"})," Test on real before deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-3-full-navigation-pipeline",children:"\ud83d\udd2c Lab Exercise 3: Full Navigation Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Deploy complete autonomy stack"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception:"})," Object detection + depth"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localization:"})," SLAM or visual odometry"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Planning:"})," Nav2 with costmaps"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control:"})," Pure pursuit or DWA"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test:"})," Navigate warehouse (sim \u2192 real)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Integration:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Isaac Sim \u2192 Perception (Isaac ROS) \u2192 SLAM \u2192 Nav2 \u2192 Robot\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Working navigation in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Deployment on Jetson (or PC with CUDA)"}),"\n",(0,r.jsx)(n.li,{children:"Video of autonomous navigation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-3-assessment-checklist",children:"\u2705 Week 3 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Apply domain randomization to training"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Deploy model on Jetson/GPU hardware"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate perception with Nav2"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test sim-to-real transfer"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Document performance gaps and solutions"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-module-summary",children:"\ud83d\udcdd Module Summary"}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-mastered",children:"Key Concepts Mastered"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Isaac Sim GPU Simulation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Photorealistic RTX rendering"}),"\n",(0,r.jsx)(n.li,{children:"PhysX 5 GPU-accelerated physics"}),"\n",(0,r.jsx)(n.li,{children:"Python API for scene creation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Synthetic Data Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Domain randomization techniques"}),"\n",(0,r.jsx)(n.li,{children:"Perfect auto-labeling with USD"}),"\n",(0,r.jsx)(n.li,{children:"Massive dataset creation (1000s of images)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Hardware-Accelerated Perception"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"TensorRT optimization"}),"\n",(0,r.jsx)(n.li,{children:"Isaac ROS GEMs deployment"}),"\n",(0,r.jsx)(n.li,{children:"Real-time inference on Jetson"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sim-to-Real Transfer"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understanding the reality gap"}),"\n",(0,r.jsx)(n.li,{children:"Techniques to bridge gap"}),"\n",(0,r.jsx)(n.li,{children:"Progressive training strategies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autonomous Warehouses:"})," Train robots on synthetic warehouse data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manipulation:"})," Pick-and-place with vision-based grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inspection:"})," Defect detection trained on CAD + randomization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Agriculture:"})," Crop detection without manual labeling"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-pitfalls--solutions",children:"Common Pitfalls & Solutions"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Problem"}),(0,r.jsx)(n.th,{children:"Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Isaac Sim crashes"}),(0,r.jsx)(n.td,{children:"Check GPU drivers (NVIDIA 525+), reduce render quality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Slow inference"}),(0,r.jsx)(n.td,{children:"Use TensorRT, reduce model size, batch images"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Poor sim-to-real transfer"}),(0,r.jsx)(n.td,{children:"Increase randomization, add sensor noise"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"USD scene too large"}),(0,r.jsx)(n.td,{children:"Use instancing, LODs, optimize assets"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Module 4: VLA (Vision-Language-Action models)\n\u2705 Deploy full autonomy stack on hardware\n\u2705 Explore Isaac Gym for RL training"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-final-project-sim-to-real-object-manipulation",children:"\ud83c\udf93 Final Project: Sim-to-Real Object Manipulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective:"})," Train a robot arm to grasp objects using only synthetic data."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulation:"})," Isaac Sim with Franka robot arm"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data:"})," Generate 10,000 images with random objects/poses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model:"})," Train grasp detector (6-DOF pose estimation)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment:"})," Deploy on real robot arm (or continue in sim)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluation:"})," Measure grasp success rate"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Grading Rubric:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Dataset quality and diversity (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Model training and optimization (25%)"}),"\n",(0,r.jsx)(n.li,{children:"TensorRT deployment (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Sim-to-real analysis (25%)"}),"\n",(0,r.jsx)(n.li,{children:"Demo video and report (10%)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Submission:"})," GitHub repo + trained model + analysis PDF + video"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Official Documentation:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"Isaac Sim Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/",children:"Isaac ROS GEMs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html",children:"Omniverse Replicator"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nvidia-omniverse.github.io/PhysX/physx/5.3.0/",children:"PhysX 5 SDK"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Video Tutorials:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/nvidia-isaac",children:"Isaac Sim Getting Started"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=synthetic-data-isaac",children:"Synthetic Data Generation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/isaac-ros",children:"Isaac ROS Deployment"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Community:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://forums.developer.nvidia.com/c/omniverse",children:"NVIDIA Omniverse Forums"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://discord.gg/nvidia-isaac",children:"Isaac Sim Discord"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"Isaac ROS GitHub"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Recommended Hardware:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Development:"})," RTX 3060+ (12GB VRAM), 32GB RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment:"})," Jetson Orin (for edge) or RTX 4000+ (datacenter)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Papers:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"'}),"\n",(0,r.jsx)(n.li,{children:'"Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience"'}),"\n",(0,r.jsx)(n.li,{children:'"NVIDIA Isaac Gym: High Performance GPU-Based Physics Simulation"'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);