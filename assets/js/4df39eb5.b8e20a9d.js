"use strict";(globalThis.webpackChunkteaching_physical_ai_robotics_book=globalThis.webpackChunkteaching_physical_ai_robotics_book||[]).push([[411],{2609:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-04/module-vla","title":"Module 4: The Cognitive Layer - Vision-Language-Action (VLA) Models","description":"\ud83d\udcda Learning Path Overview","source":"@site/docs/chapter-04/module-vla.md","sourceDirName":"chapter-04","slug":"/chapter-04/module-vla","permalink":"/humanoid-robotics-book/docs/chapter-04/module-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Aisha-Sarfaraz/humanoid-robotics-book/tree/main/docs/chapter-04/module-vla.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac Platform)","permalink":"/humanoid-robotics-book/docs/chapter-04/module-isaac"},"next":{"title":"4.6 Assessment Philosophy","permalink":"/humanoid-robotics-book/docs/chapter-04/assessment"}}');var t=s(4848),o=s(8453);const r={},a="Module 4: The Cognitive Layer - Vision-Language-Action (VLA) Models",l={},c=[{value:"\ud83d\udcda Learning Path Overview",id:"-learning-path-overview",level:2},{value:"Learning Progression",id:"learning-progression",level:3},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Week 1: Vision Foundation Models for Robotics",id:"week-1-vision-foundation-models-for-robotics",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:3},{value:"\ud83d\udcd6 Theory: From Computer Vision to Embodied Vision",id:"-theory-from-computer-vision-to-embodied-vision",level:3},{value:"\ud83d\udcbb Code Example 1: CLIP-Based Visual Grounding for Robot Perception",id:"-code-example-1-clip-based-visual-grounding-for-robot-perception",level:3},{value:"\ud83d\udd2c Lab Exercise 1: Visual Question Answering",id:"-lab-exercise-1-visual-question-answering",level:3},{value:"\u2705 Assessment Checklist - Week 1",id:"-assessment-checklist---week-1",level:3},{value:"Week 2: Language Models for Task Planning",id:"week-2-language-models-for-task-planning",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-1",level:3},{value:"\ud83d\udcd6 Theory: LLMs as Robot Task Planners",id:"-theory-llms-as-robot-task-planners",level:3},{value:"\ud83d\udcbb Code Example 2: LLM Task Planner for Robot Commands",id:"-code-example-2-llm-task-planner-for-robot-commands",level:3},{value:"\ud83d\udcbb Code Example 3: Natural Language Command Parser with Error Handling",id:"-code-example-3-natural-language-command-parser-with-error-handling",level:3},{value:"\ud83d\udd2c Lab Exercise 2: Multi-Step Task Execution",id:"-lab-exercise-2-multi-step-task-execution",level:3},{value:"\u2705 Assessment Checklist - Week 2",id:"-assessment-checklist---week-2",level:3},{value:"Week 3: End-to-End VLA Integration",id:"week-3-end-to-end-vla-integration",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-2",level:3},{value:"\ud83d\udcd6 Theory: Vision-Language-Action Models",id:"-theory-vision-language-action-models",level:3},{value:"\ud83d\udcbb Code Example 4: End-to-End VLA Pipeline",id:"-code-example-4-end-to-end-vla-pipeline",level:3},{value:"\ud83d\udd2c Lab Exercise 3: VLA Data Collection and Fine-Tuning",id:"-lab-exercise-3-vla-data-collection-and-fine-tuning",level:3},{value:"\u2705 Assessment Checklist - Week 3",id:"-assessment-checklist---week-3",level:3},{value:"\ud83d\udcdd Module Summary",id:"-module-summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Skills Mastered",id:"skills-mastered",level:3},{value:"Common Pitfalls",id:"common-pitfalls",level:3},{value:"\ud83c\udf93 Final Project: Multimodal Voice-Controlled Robot",id:"-final-project-multimodal-voice-controlled-robot",level:2},{value:"Project Requirements",id:"project-requirements",level:3},{value:"Example Workflow",id:"example-workflow",level:3},{value:"Evaluation Rubric",id:"evaluation-rubric",level:3},{value:"Deliverables",id:"deliverables",level:3},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2},{value:"Papers",id:"papers",level:3},{value:"Code Repositories",id:"code-repositories",level:3},{value:"Datasets",id:"datasets",level:3},{value:"Tools",id:"tools",level:3},{value:"Online Courses",id:"online-courses",level:3},{value:"\ud83d\udd17 Integration with Previous Modules",id:"-integration-with-previous-modules",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-the-cognitive-layer---vision-language-action-vla-models",children:"Module 4: The Cognitive Layer - Vision-Language-Action (VLA) Models"})}),"\n",(0,t.jsx)(n.h2,{id:"-learning-path-overview",children:"\ud83d\udcda Learning Path Overview"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 3 weeks\n",(0,t.jsx)(n.strong,{children:"Difficulty"}),": Advanced\n",(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Modules 1-3 (ROS 2, Simulation, NVIDIA Isaac)"]}),"\n",(0,t.jsx)(n.h3,{id:"learning-progression",children:"Learning Progression"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Week 1: Vision Foundation Models\n  \u2514\u2500> CLIP, Visual Transformers, Object Recognition\n       \u2514\u2500> Week 2: Language Models for Robotics\n            \u2514\u2500> LLM Task Planning, Natural Language Interface\n                 \u2514\u2500> Week 3: VLA Integration\n                      \u2514\u2500> Multimodal Fusion, End-to-End Control\n"})}),"\n",(0,t.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Integrate vision foundation models (CLIP, DINO) with ROS 2"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Use LLMs for high-level task planning and decomposition"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Build natural language interfaces for robot control"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Implement end-to-end VLA pipelines (RT-1/RT-2 style)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Handle multimodal sensor fusion (vision + language + proprioception)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Deploy vision-language models with TensorRT optimization"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"week-1-vision-foundation-models-for-robotics",children:"Week 1: Vision Foundation Models for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand vision transformers and contrastive learning (CLIP)"}),"\n",(0,t.jsx)(n.li,{children:"Implement visual grounding for natural language queries"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision models with ROS 2 perception stack"}),"\n",(0,t.jsx)(n.li,{children:"Optimize vision models for real-time robot control"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-theory-from-computer-vision-to-embodied-vision",children:"\ud83d\udcd6 Theory: From Computer Vision to Embodied Vision"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Traditional Computer Vision vs. Foundation Models"})}),"\n",(0,t.jsx)(n.p,{children:"Traditional robotics vision relied on task-specific models:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Object detection: Trained on specific object classes"}),"\n",(0,t.jsx)(n.li,{children:"Semantic segmentation: Fixed categories (person, car, chair)"}),"\n",(0,t.jsx)(n.li,{children:"Pose estimation: Predefined keypoint sets"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision Foundation Models"})," change the paradigm:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLIP"})," (Contrastive Language-Image Pre-training): Zero-shot object recognition using natural language descriptions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DINO"})," (Self-Distillation with No Labels): Self-supervised visual features without human labels"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SAM"})," (Segment Anything Model): Universal segmentation with prompt engineering"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Insight"}),': Instead of training a new model to detect "red cups," you can query CLIP with "a red cup on a table" and get semantic similarity scores.']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Architecture Overview - CLIP"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Text: "a red cup"  \u2500\u2500\u2510\n                     \u2502\nImage: [RGB frame]  \u2500\u2500\u253c\u2500\u2500> Cosine Similarity \u2500\u2500> Match Score\n                     \u2502\nVision Encoder       Text Encoder\n(ViT-B/32)          (Transformer)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why This Matters for Robotics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open vocabulary"}),": Recognize objects not in training set"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language grounding"}),": Natural language queries instead of class IDs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Faster deployment"}),": No need to collect/label robot-specific datasets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal reasoning"}),": Bridge vision and language for task understanding"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-code-example-1-clip-based-visual-grounding-for-robot-perception",children:"\ud83d\udcbb Code Example 1: CLIP-Based Visual Grounding for Robot Perception"}),"\n",(0,t.jsx)(n.p,{children:"This ROS 2 node uses OpenAI's CLIP to perform zero-shot object detection from natural language queries."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nCLIP Visual Grounding Node\nSubscribes to camera images, uses CLIP to find objects matching text queries.\nPublishes detected object locations for manipulation or navigation.\n\nDependencies:\n  pip install transformers torch torchvision pillow\n  sudo apt install ros-humble-cv-bridge ros-humble-vision-msgs\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Pose2D\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image as PILImage\n\nclass CLIPGroundingNode(Node):\n    def __init__(self):\n        super().__init__(\'clip_grounding_node\')\n\n        # Parameters\n        self.declare_parameter(\'model_name\', \'openai/clip-vit-base-patch32\')\n        self.declare_parameter(\'text_queries\', [\'a red cup\', \'a blue bottle\', \'a laptop\'])\n        self.declare_parameter(\'confidence_threshold\', 0.25)\n        self.declare_parameter(\'grid_size\', 16)  # Divide image into 16x16 grid\n\n        model_name = self.get_parameter(\'model_name\').value\n        self.text_queries = self.get_parameter(\'text_queries\').value\n        self.confidence_threshold = self.get_parameter(\'confidence_threshold\').value\n        self.grid_size = self.get_parameter(\'grid_size\').value\n\n        # Load CLIP model\n        self.get_logger().info(f\'Loading CLIP model: {model_name}\')\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model.eval()\n\n        # Precompute text embeddings for queries\n        self.text_features = self._encode_text_queries(self.text_queries)\n\n        # ROS 2 setup\n        self.bridge = CvBridge()\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.image_callback, 10\n        )\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, \'/clip/detections\', 10\n        )\n\n        self.get_logger().info(\'CLIP Grounding Node initialized\')\n        self.get_logger().info(f\'Queries: {self.text_queries}\')\n\n    def _encode_text_queries(self, queries):\n        """Precompute and cache text embeddings"""\n        inputs = self.processor(text=queries, return_tensors="pt", padding=True)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            text_features = self.model.get_text_features(**inputs)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        return text_features\n\n    def image_callback(self, msg):\n        """Process incoming camera images"""\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n        h, w = cv_image.shape[:2]\n\n        # Sliding window approach: divide image into overlapping crops\n        detections = Detection2DArray()\n        detections.header = msg.header\n\n        step_size = max(h, w) // self.grid_size\n        window_size = step_size * 2  # Overlapping windows\n\n        for y in range(0, h - window_size, step_size):\n            for x in range(0, w - window_size, step_size):\n                crop = cv_image[y:y+window_size, x:x+window_size]\n\n                # Get CLIP similarity scores for this crop\n                similarities = self._compute_similarities(crop)\n\n                # Find best matching query\n                max_score_idx = similarities.argmax().item()\n                max_score = similarities[max_score_idx].item()\n\n                if max_score > self.confidence_threshold:\n                    detection = self._create_detection(\n                        x, y, window_size, window_size,\n                        self.text_queries[max_score_idx],\n                        max_score\n                    )\n                    detections.detections.append(detection)\n\n        # Apply non-maximum suppression to remove overlapping detections\n        detections.detections = self._nms(detections.detections, iou_threshold=0.5)\n\n        self.detection_pub.publish(detections)\n        self.get_logger().info(f\'Published {len(detections.detections)} detections\')\n\n    def _compute_similarities(self, crop_np):\n        """Compute CLIP similarity between crop and text queries"""\n        # Convert numpy to PIL\n        crop_pil = PILImage.fromarray(crop_np)\n\n        # Process image\n        inputs = self.processor(images=crop_pil, return_tensors="pt")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # Get image features\n        with torch.no_grad():\n            image_features = self.model.get_image_features(**inputs)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n\n        # Compute cosine similarity\n        similarities = (image_features @ self.text_features.T).squeeze(0)\n        return similarities\n\n    def _create_detection(self, x, y, width, height, label, score):\n        """Create Detection2D message"""\n        detection = Detection2D()\n\n        # Bounding box center\n        detection.bbox.center.position.x = float(x + width / 2)\n        detection.bbox.center.position.y = float(y + height / 2)\n        detection.bbox.size_x = float(width)\n        detection.bbox.size_y = float(height)\n\n        # Hypothesis (label + confidence)\n        hypothesis = ObjectHypothesisWithPose()\n        hypothesis.hypothesis.class_id = label\n        hypothesis.hypothesis.score = float(score)\n        detection.results.append(hypothesis)\n\n        return detection\n\n    def _nms(self, detections, iou_threshold=0.5):\n        """Non-maximum suppression to remove overlapping detections"""\n        if len(detections) == 0:\n            return detections\n\n        # Extract bounding boxes and scores\n        boxes = []\n        scores = []\n        for det in detections:\n            x = det.bbox.center.position.x - det.bbox.size_x / 2\n            y = det.bbox.center.position.y - det.bbox.size_y / 2\n            boxes.append([x, y, x + det.bbox.size_x, y + det.bbox.size_y])\n            scores.append(det.results[0].hypothesis.score)\n\n        boxes = np.array(boxes)\n        scores = np.array(scores)\n\n        # Apply NMS\n        indices = self._nms_boxes(boxes, scores, iou_threshold)\n        return [detections[i] for i in indices]\n\n    def _nms_boxes(self, boxes, scores, iou_threshold):\n        """Pure NumPy NMS implementation"""\n        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n        areas = (x2 - x1) * (y2 - y1)\n        order = scores.argsort()[::-1]\n\n        keep = []\n        while order.size > 0:\n            i = order[0]\n            keep.append(i)\n\n            # Compute IoU with remaining boxes\n            xx1 = np.maximum(x1[i], x1[order[1:]])\n            yy1 = np.maximum(y1[i], y1[order[1:]])\n            xx2 = np.minimum(x2[i], x2[order[1:]])\n            yy2 = np.minimum(y2[i], y2[order[1:]])\n\n            w = np.maximum(0.0, xx2 - xx1)\n            h = np.maximum(0.0, yy2 - yy1)\n            inter = w * h\n\n            iou = inter / (areas[i] + areas[order[1:]] - inter)\n\n            # Keep boxes with IoU below threshold\n            order = order[np.where(iou <= iou_threshold)[0] + 1]\n\n        return keep\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CLIPGroundingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Technical Details"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sliding window"}),": Divides image into overlapping crops to localize objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Precomputed text embeddings"}),": Encodes text queries once for efficiency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Non-maximum suppression"}),": Removes duplicate detections"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-shot"}),": No training required, works with any text description"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch this node"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 run my_robot_pkg clip_grounding_node --ros-args \\\n  -p text_queries:=\"['a red cup', 'a person', 'a door handle']\" \\\n  -p confidence_threshold:=0.3\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-lab-exercise-1-visual-question-answering",children:"\ud83d\udd2c Lab Exercise 1: Visual Question Answering"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Extend the CLIP node to answer visual questions like:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Is there a red object on the table?"'}),"\n",(0,t.jsx)(n.li,{children:'"How many cups are visible?"'}),"\n",(0,t.jsx)(n.li,{children:'"What color is the nearest bottle?"'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Use multiple text prompts: "a red object", "a blue object", etc.'}),"\n",(0,t.jsx)(n.li,{children:'Count detections above threshold for "how many" questions'}),"\n",(0,t.jsx)(n.li,{children:"Use spatial reasoning: nearest object = largest bounding box area"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-assessment-checklist---week-1",children:"\u2705 Assessment Checklist - Week 1"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","CLIP model loads and processes images in under 200ms"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visual grounding detects objects with over 70% accuracy on test queries"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Non-maximum suppression correctly removes duplicate detections"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Node publishes Detection2DArray messages at camera frame rate"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Zero-shot queries work without retraining (test with novel objects)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"week-2-language-models-for-task-planning",children:"Week 2: Language Models for Task Planning"}),"\n",(0,t.jsx)(n.h3,{id:"-learning-objectives-1",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use LLMs to decompose high-level commands into robot primitives"}),"\n",(0,t.jsx)(n.li,{children:"Implement prompt engineering for robotics tasks"}),"\n",(0,t.jsx)(n.li,{children:"Build natural language command parsing with error handling"}),"\n",(0,t.jsx)(n.li,{children:"Translate LLM outputs to ROS 2 action sequences"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-theory-llms-as-robot-task-planners",children:"\ud83d\udcd6 Theory: LLMs as Robot Task Planners"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"The Challenge of Natural Language Commands"})}),"\n",(0,t.jsx)(n.p,{children:"Traditional robot programming:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"robot.navigate_to('kitchen')\nrobot.detect_object('red_cup')\nrobot.grasp('red_cup')\n"})}),"\n",(0,t.jsx)(n.p,{children:"Natural language request:"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:'"Hey robot, could you grab me that red cup from the kitchen counter?"'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLMs bridge this gap"})," by:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understanding intent"}),': "grab" \u2192 grasp action']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extracting entities"}),': "red cup", "kitchen counter"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task decomposition"}),": Navigate \u2192 Detect \u2192 Grasp \u2192 Return"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handling ambiguity"}),': "that red cup" requires visual context']}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Prompt Engineering for Robotics"})}),"\n",(0,t.jsx)(n.p,{children:"The quality of LLM output depends heavily on prompt structure:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bad Prompt"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"User: Go to the kitchen and grab the red cup\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Good Prompt"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'You are a robot task planner. Decompose the user\'s request into a sequence\nof primitive actions from this list:\n- navigate(location: str)\n- detect(object: str)\n- grasp(object: str)\n- place(location: str)\n- say(message: str)\n\nUser request: "Go to the kitchen and grab the red cup"\n\nOutput format (JSON):\n{\n  "plan": [\n    {"action": "navigate", "params": {"location": "kitchen"}},\n    {"action": "detect", "params": {"object": "red cup"}},\n    ...\n  ],\n  "assumptions": ["Kitchen location is known in map", "Red cup is visible"],\n  "potential_failures": ["Cup not found", "Navigation blocked"]\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Principles"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Constrain output format"}),": JSON, YAML, or structured text"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define primitive actions"}),": Finite action vocabulary"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request assumptions/failures"}),": Forces reasoning about edge cases"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Few-shot examples"}),": Include 2-3 example decompositions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-code-example-2-llm-task-planner-for-robot-commands",children:"\ud83d\udcbb Code Example 2: LLM Task Planner for Robot Commands"}),"\n",(0,t.jsx)(n.p,{children:"This node uses OpenAI's GPT-4 to decompose natural language commands into executable ROS 2 action sequences."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nLLM Task Planner Node\nReceives natural language commands via service, uses GPT-4 to generate\naction plans, and publishes to execution coordinator.\n\nDependencies:\n  pip install openai pydantic\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom example_interfaces.srv import SetString\nimport openai\nimport json\nfrom typing import List, Dict, Optional\nfrom pydantic import BaseModel, Field\n\n# Define structured output schema\nclass ActionPrimitive(BaseModel):\n    action: str = Field(..., description="Action name from predefined set")\n    params: Dict[str, str] = Field(..., description="Action parameters")\n    timeout: float = Field(10.0, description="Max execution time in seconds")\n\nclass TaskPlan(BaseModel):\n    plan: List[ActionPrimitive]\n    assumptions: List[str]\n    potential_failures: List[str]\n    estimated_duration: float\n\nclass LLMTaskPlanner(Node):\n    def __init__(self):\n        super().__init__(\'llm_task_planner\')\n\n        # Parameters\n        self.declare_parameter(\'openai_api_key\', \'\')\n        self.declare_parameter(\'model\', \'gpt-4\')\n        self.declare_parameter(\'temperature\', 0.2)  # Low temperature for consistent output\n\n        api_key = self.get_parameter(\'openai_api_key\').value\n        if not api_key:\n            self.get_logger().error(\'OpenAI API key not provided!\')\n            return\n\n        openai.api_key = api_key\n        self.model = self.get_parameter(\'model\').value\n        self.temperature = self.get_parameter(\'temperature\').value\n\n        # Define robot\'s action primitives\n        self.action_primitives = {\n            \'navigate\': \'Move to a named location (params: location)\',\n            \'detect\': \'Detect objects matching description (params: object_description)\',\n            \'grasp\': \'Grasp detected object (params: object_id)\',\n            \'place\': \'Place held object at location (params: location)\',\n            \'say\': \'Speak text via TTS (params: message)\',\n            \'wait\': \'Wait for duration (params: duration_sec)\',\n        }\n\n        # ROS 2 interfaces\n        self.plan_service = self.create_service(\n            SetString, \'/robot/plan_task\', self.plan_task_callback\n        )\n        self.plan_pub = self.create_publisher(String, \'/robot/task_plan\', 10)\n\n        self.get_logger().info(\'LLM Task Planner initialized\')\n        self.get_logger().info(f\'Available actions: {list(self.action_primitives.keys())}\')\n\n    def plan_task_callback(self, request, response):\n        """Service callback: generate plan from natural language command"""\n        user_command = request.data\n        self.get_logger().info(f\'Planning task: "{user_command}"\')\n\n        try:\n            plan = self._generate_plan(user_command)\n            response.success = True\n            response.message = json.dumps(plan.dict(), indent=2)\n\n            # Publish plan for execution\n            plan_msg = String()\n            plan_msg.data = response.message\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f\'Generated plan with {len(plan.plan)} steps\')\n        except Exception as e:\n            response.success = False\n            response.message = f\'Planning failed: {str(e)}\'\n            self.get_logger().error(response.message)\n\n        return response\n\n    def _generate_plan(self, user_command: str) -> TaskPlan:\n        """Use GPT-4 to decompose command into action sequence"""\n\n        # Construct system prompt\n        system_prompt = self._build_system_prompt()\n\n        # Construct user prompt with few-shot examples\n        user_prompt = f"""\nUser command: "{user_command}"\n\nGenerate a task plan following the output schema.\n"""\n\n        # Call OpenAI API\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": "Go to the living room and turn on the lights"},\n                {"role": "assistant", "content": self._example_plan_1()},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=self.temperature,\n            max_tokens=500\n        )\n\n        # Parse response\n        plan_text = response.choices[0].message.content\n        plan_dict = json.loads(plan_text)\n        plan = TaskPlan(**plan_dict)\n\n        # Validate all actions are in primitive set\n        for step in plan.plan:\n            if step.action not in self.action_primitives:\n                raise ValueError(f\'Invalid action: {step.action}\')\n\n        return plan\n\n    def _build_system_prompt(self) -> str:\n        """Construct system prompt defining robot\'s capabilities"""\n        actions_desc = \'\\n\'.join([\n            f"- {name}: {desc}" for name, desc in self.action_primitives.items()\n        ])\n\n        return f"""You are a task planning system for a mobile manipulator robot.\n\nYour job is to decompose natural language commands into sequences of primitive actions.\n\nAvailable primitive actions:\n{actions_desc}\n\nOutput format (JSON):\n{{\n  "plan": [\n    {{"action": "navigate", "params": {{"location": "kitchen"}}, "timeout": 30.0}},\n    {{"action": "detect", "params": {{"object_description": "red cup"}}, "timeout": 10.0}},\n    ...\n  ],\n  "assumptions": ["List of assumptions about environment/objects"],\n  "potential_failures": ["Possible failure modes"],\n  "estimated_duration": 45.0\n}}\n\nImportant:\n- Only use actions from the predefined list\n- Be specific in object descriptions for detect actions\n- Include reasonable timeouts (navigate: 30s, detect: 10s, grasp: 15s)\n- List assumptions (e.g., "Kitchen location known in map")\n- Consider failure cases (e.g., "Object not found")\n- Estimate total duration based on timeouts\n"""\n\n    def _example_plan_1(self) -> str:\n        """Few-shot example for prompt"""\n        return json.dumps({\n            "plan": [\n                {"action": "navigate", "params": {"location": "living_room"}, "timeout": 30.0},\n                {"action": "detect", "params": {"object_description": "light switch"}, "timeout": 10.0},\n                {"action": "say", "params": {"message": "Lights turned on"}, "timeout": 2.0}\n            ],\n            "assumptions": [\n                "Living room location is in the map",\n                "Light switch is visible and detectable",\n                "Robot has capability to interact with switches (simplified)"\n            ],\n            "potential_failures": [\n                "Navigation blocked by obstacles",\n                "Light switch not detected",\n                "Switch already on"\n            ],\n            "estimated_duration": 42.0\n        }, indent=2)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMTaskPlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Test the planner"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the planner node\nros2 run my_robot_pkg llm_task_planner --ros-args \\\n  -p openai_api_key:=$OPENAI_API_KEY\n\n# Terminal 2: Send a planning request\nros2 service call /robot/plan_task example_interfaces/srv/SetString \\\n  \"{data: 'Go to the kitchen and grab the red cup from the counter'}\"\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-code-example-3-natural-language-command-parser-with-error-handling",children:"\ud83d\udcbb Code Example 3: Natural Language Command Parser with Error Handling"}),"\n",(0,t.jsx)(n.p,{children:"This node adds conversational error handling and clarification dialogs."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nConversational Command Parser\nHandles ambiguous commands by asking clarification questions.\n\nExample:\n  User: "Grab the cup"\n  Robot: "I see 3 cups - red, blue, and green. Which one?"\n  User: "The red one"\n  Robot: "Got it, executing: grasp red cup"\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom example_interfaces.srv import SetString\nimport openai\nimport json\nfrom enum import Enum\n\nclass ConversationState(Enum):\n    IDLE = 1\n    AWAITING_CLARIFICATION = 2\n    EXECUTING = 3\n\nclass ConversationalParser(Node):\n    def __init__(self):\n        super().__init__(\'conversational_parser\')\n\n        # Parameters\n        self.declare_parameter(\'openai_api_key\', \'\')\n        api_key = self.get_parameter(\'openai_api_key\').value\n        openai.api_key = api_key\n\n        # Conversation state\n        self.state = ConversationState.IDLE\n        self.conversation_history = []\n        self.pending_command = None\n        self.detected_objects = []  # Simulated perception state\n\n        # ROS 2 interfaces\n        self.command_service = self.create_service(\n            SetString, \'/robot/voice_command\', self.command_callback\n        )\n        self.response_pub = self.create_publisher(String, \'/robot/response\', 10)\n\n        # Subscribe to perception updates\n        self.create_subscription(String, \'/clip/detected_objects\',\n                                self.perception_callback, 10)\n\n        self.get_logger().info(\'Conversational Parser ready\')\n\n    def perception_callback(self, msg):\n        """Update list of detected objects"""\n        self.detected_objects = json.loads(msg.data)\n\n    def command_callback(self, request, response):\n        """Process voice command with context awareness"""\n        user_input = request.data\n        self.get_logger().info(f\'Received: "{user_input}"\')\n\n        # Add to conversation history\n        self.conversation_history.append({"role": "user", "content": user_input})\n\n        try:\n            # Check if this is a clarification response\n            if self.state == ConversationState.AWAITING_CLARIFICATION:\n                result = self._handle_clarification(user_input)\n            else:\n                result = self._handle_new_command(user_input)\n\n            response.success = result[\'success\']\n            response.message = result[\'message\']\n\n            # Publish robot\'s response\n            response_msg = String()\n            response_msg.data = result[\'message\']\n            self.response_pub.publish(response_msg)\n\n        except Exception as e:\n            response.success = False\n            response.message = f\'Error: {str(e)}\'\n            self.get_logger().error(response.message)\n\n        return response\n\n    def _handle_new_command(self, command: str) -> dict:\n        """Process a new command, checking for ambiguities"""\n\n        # Analyze command with current perception context\n        analysis = self._analyze_command_with_context(command)\n\n        if analysis[\'is_ambiguous\']:\n            # Need clarification\n            self.state = ConversationState.AWAITING_CLARIFICATION\n            self.pending_command = command\n            return {\n                \'success\': False,\n                \'message\': analysis[\'clarification_question\']\n            }\n        else:\n            # Clear command, can execute\n            self.state = ConversationState.EXECUTING\n            return {\n                \'success\': True,\n                \'message\': f"Executing: {analysis[\'action_plan\']}"\n            }\n\n    def _handle_clarification(self, response: str) -> dict:\n        """Process user\'s clarification response"""\n\n        # Resolve ambiguity using clarification\n        resolved_command = self._resolve_ambiguity(self.pending_command, response)\n\n        self.state = ConversationState.IDLE\n        self.pending_command = None\n\n        return {\n            \'success\': True,\n            \'message\': f"Got it, executing: {resolved_command}"\n        }\n\n    def _analyze_command_with_context(self, command: str) -> dict:\n        """Use LLM to analyze command given current perception state"""\n\n        system_prompt = f"""You are a robot command analyzer.\nCurrent visible objects: {json.dumps(self.detected_objects)}\n\nAnalyze if the user\'s command is ambiguous given what the robot can see.\n\nIf ambiguous, generate a clarification question.\nIf clear, generate the action plan.\n\nOutput JSON:\n{{\n  "is_ambiguous": true/false,\n  "reason": "explanation",\n  "clarification_question": "question to ask user" (if ambiguous),\n  "action_plan": "what to execute" (if not ambiguous)\n}}\n"""\n\n        response = openai.ChatCompletion.create(\n            model=\'gpt-4\',\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": command}\n            ],\n            temperature=0.3\n        )\n\n        return json.loads(response.choices[0].message.content)\n\n    def _resolve_ambiguity(self, original_command: str, clarification: str) -> str:\n        """Combine original command with clarification"""\n\n        system_prompt = """Combine the original ambiguous command with the user\'s\nclarification into a clear, executable command."""\n\n        user_prompt = f"""\nOriginal: "{original_command}"\nClarification: "{clarification}"\n\nOutput the resolved command.\n"""\n\n        response = openai.ChatCompletion.create(\n            model=\'gpt-4\',\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.2\n        )\n\n        return response.choices[0].message.content\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ConversationalParser()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Conversation Example"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'$ ros2 service call /robot/voice_command example_interfaces/srv/SetString \\\n  "{data: \'Grab the cup\'}"\n\nResponse: "I see 3 cups - red, blue, and green. Which one should I grab?"\n\n$ ros2 service call /robot/voice_command example_interfaces/srv/SetString \\\n  "{data: \'The red one near the laptop\'}"\n\nResponse: "Got it, executing: grasp red cup at (x: 0.45, y: 0.32)"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-lab-exercise-2-multi-step-task-execution",children:"\ud83d\udd2c Lab Exercise 2: Multi-Step Task Execution"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Implement a task executor that runs the LLM-generated plans from Code Example 2."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Subscribe to ",(0,t.jsx)(n.code,{children:"/robot/task_plan"})," topic"]}),"\n",(0,t.jsxs)(n.li,{children:["For each action in the plan:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Call corresponding ROS 2 action server (navigate, detect, grasp)"}),"\n",(0,t.jsx)(n.li,{children:"Handle timeouts and failures"}),"\n",(0,t.jsx)(n.li,{children:"If step fails, ask LLM to replan from current state"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Publish progress updates to ",(0,t.jsx)(n.code,{children:"/robot/task_progress"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use ",(0,t.jsx)(n.code,{children:"rclpy.action.ActionClient"})," for each primitive action"]}),"\n",(0,t.jsxs)(n.li,{children:["Implement error recovery: if ",(0,t.jsx)(n.code,{children:"detect"})," fails, call ",(0,t.jsx)(n.code,{children:"navigate"})," to search location"]}),"\n",(0,t.jsx)(n.li,{children:"Log all steps for debugging"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-assessment-checklist---week-2",children:"\u2705 Assessment Checklist - Week 2"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","LLM planner decomposes commands into valid action sequences"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Prompt engineering constrains output to defined primitives"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Conversational parser handles at least 3 types of ambiguity"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Error handling gracefully recovers from LLM API failures"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Task executor successfully runs multi-step plans (over 80% success rate)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"week-3-end-to-end-vla-integration",children:"Week 3: End-to-End VLA Integration"}),"\n",(0,t.jsx)(n.h3,{id:"-learning-objectives-2",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand VLA model architectures (RT-1, RT-2)"}),"\n",(0,t.jsx)(n.li,{children:"Implement multimodal sensor fusion (vision + language + proprioception)"}),"\n",(0,t.jsx)(n.li,{children:"Deploy end-to-end VLA policies for robot control"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate sim-to-real transfer for VLA models"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-theory-vision-language-action-models",children:"\ud83d\udcd6 Theory: Vision-Language-Action Models"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What is a VLA Model?"})}),"\n",(0,t.jsx)(n.p,{children:"Traditional robotics pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Perception \u2192 Task Planning \u2192 Motion Planning \u2192 Control\n(separate models for each stage)\n"})}),"\n",(0,t.jsx)(n.p,{children:"VLA models unify this into a single neural network:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Image + Language Command + Robot State]\n         \u2193\n   VLA Transformer\n         \u2193\n   [Action Outputs: joint velocities, gripper state]\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key VLA Architectures"}),":"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"RT-1"})," (Robotics Transformer 1):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input"}),": Image (300x300 RGB) + Text command + Robot state (joint angles)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Architecture"}),": Vision Transformer (ViT) for image encoding, concatenated with text embeddings and robot state, fed to Transformer decoder"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": Discretized action bins (e.g., 256 bins for each joint)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Training"}),": Imitation learning on 130k robot demonstrations"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"RT-2"})," (Robotics Transformer 2):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Built on PaLM-E (vision-language model)"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Key innovation"}),": Pretrains on web-scale vision-language data, fine-tunes on robot data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergent capabilities"}),": Generalizes to objects never seen in robot training (zero-shot transfer from internet knowledge)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Fusion Architecture"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RGB Camera  \u2502\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Depth Cam   \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2192\u2502 Vision Encoder\u2502\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                      \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Wrist Cam   \u2502\u2500\u2500\u2518                      \u251c\u2500\u2500\u2500\u2192\u2502  Transformer\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502    \u2502   Backbone  \u2502\u2500\u2500\u2192 Actions\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502"Pick red cup"\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 Text Encoder \u2502\u2500\u2524\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 Joint angles\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502  MLP Encoder \u2502\u2500\u2518\n\u2502 Gripper state\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why VLA Models Matter"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simplified deployment"}),": One model instead of perception + planning + control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergent reasoning"}),": Language grounding enables zero-shot generalization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-end optimization"}),": Gradients flow through entire pipeline"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Can leverage massive vision-language datasets"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-code-example-4-end-to-end-vla-pipeline",children:"\ud83d\udcbb Code Example 4: End-to-End VLA Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"This example demonstrates a simplified VLA model for pick-and-place tasks, inspired by RT-1."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSimplified VLA Model for Robot Control\nIntegrates vision (camera), language (task description), and proprioception\n(robot state) to output low-level actions.\n\nNote: This is a teaching example. Production VLA models like RT-1/RT-2 require\nextensive datasets and compute. This demonstrates the architecture.\n\nDependencies:\n  pip install torch torchvision transformers einops\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPModel, CLIPProcessor\nimport numpy as np\nfrom typing import Dict, List\nimport cv2\n\nclass VLAModel(nn.Module):\n    """\n    Vision-Language-Action Transformer\n\n    Architecture:\n      1. Vision encoder: CLIP ViT-B/32 (frozen)\n      2. Language encoder: CLIP text encoder (frozen)\n      3. Proprioception encoder: MLP\n      4. Fusion: Concatenate embeddings\n      5. Policy head: Transformer \u2192 Action outputs\n    """\n\n    def __init__(self, action_dim=7, hidden_dim=512):\n        super().__init__()\n\n        # Vision-language encoders (pretrained, frozen)\n        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n        # Freeze CLIP weights\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n\n        # Proprioception encoder (robot joint states)\n        self.proprio_encoder = nn.Sequential(\n            nn.Linear(14, hidden_dim),  # 7 joints \xd7 2 (position + velocity)\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        # Fusion and policy network\n        # CLIP vision: 512-dim, CLIP text: 512-dim, proprio: 512-dim \u2192 total 1536-dim\n        self.fusion_dim = 512 * 3\n\n        # Transformer policy head\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.fusion_dim,\n            nhead=8,\n            dim_feedforward=2048,\n            batch_first=True\n        )\n        self.policy_transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n\n        # Action head (discretized actions for simplicity)\n        self.action_head = nn.Sequential(\n            nn.Linear(self.fusion_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim * 256)  # 256 bins per action dimension\n        )\n\n        self.action_dim = action_dim\n\n    def forward(self, image, text, proprio_state):\n        """\n        Args:\n            image: (B, 3, H, W) RGB image\n            text: List[str] - task descriptions\n            proprio_state: (B, 14) joint positions and velocities\n\n        Returns:\n            action_logits: (B, action_dim, 256) - discretized action probabilities\n        """\n        batch_size = image.size(0)\n        device = image.device\n\n        # 1. Encode vision\n        vision_features = self.clip_model.get_image_features(pixel_values=image)\n        # (B, 512)\n\n        # 2. Encode language\n        text_inputs = self.clip_processor(text=text, return_tensors="pt", padding=True)\n        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n        text_features = self.clip_model.get_text_features(**text_inputs)\n        # (B, 512)\n\n        # 3. Encode proprioception\n        proprio_features = self.proprio_encoder(proprio_state)\n        # (B, 512)\n\n        # 4. Fuse all modalities\n        fused_features = torch.cat([vision_features, text_features, proprio_features], dim=-1)\n        # (B, 1536)\n\n        # Add sequence dimension for transformer (treat as single token)\n        fused_features = fused_features.unsqueeze(1)  # (B, 1, 1536)\n\n        # 5. Policy transformer\n        policy_features = self.policy_transformer(fused_features)\n        # (B, 1, 1536)\n\n        policy_features = policy_features.squeeze(1)  # (B, 1536)\n\n        # 6. Action head\n        action_logits = self.action_head(policy_features)\n        # (B, action_dim * 256)\n\n        # Reshape to (B, action_dim, 256)\n        action_logits = action_logits.view(batch_size, self.action_dim, 256)\n\n        return action_logits\n\n    def predict_action(self, image, text, proprio_state):\n        """\n        Predict continuous actions from discretized logits\n\n        Returns:\n            action: (B, action_dim) continuous action values in [-1, 1]\n        """\n        with torch.no_grad():\n            action_logits = self.forward(image, text, proprio_state)\n\n            # Get most likely bin for each action dimension\n            action_bins = action_logits.argmax(dim=-1)  # (B, action_dim)\n\n            # Convert bins to continuous values: [0, 255] \u2192 [-1, 1]\n            actions = (action_bins.float() / 255.0) * 2.0 - 1.0\n\n            return actions\n\nclass VLAControlNode(Node):\n    """ROS 2 node that runs VLA model for robot control"""\n\n    def __init__(self):\n        super().__init__(\'vla_control_node\')\n\n        # Parameters\n        self.declare_parameter(\'model_path\', \'\')\n        self.declare_parameter(\'control_frequency\', 10.0)\n        self.declare_parameter(\'device\', \'cuda\')\n\n        model_path = self.get_parameter(\'model_path\').value\n        control_freq = self.get_parameter(\'control_frequency\').value\n        device = self.get_parameter(\'device\').value\n\n        # Initialize VLA model\n        self.device = torch.device(device if torch.cuda.is_available() else \'cpu\')\n        self.model = VLAModel(action_dim=7).to(self.device)\n\n        if model_path:\n            self.get_logger().info(f\'Loading model from {model_path}\')\n            self.model.load_state_dict(torch.load(model_path))\n        else:\n            self.get_logger().warn(\'No model path provided, using untrained model\')\n\n        self.model.eval()\n\n        # ROS 2 state\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.current_joints = None\n        self.current_task = "pick up the red block"  # Default task\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.image_callback, 10\n        )\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_callback, 10\n        )\n        self.task_sub = self.create_subscription(\n            String, \'/task_command\', self.task_callback, 10\n        )\n\n        # Publishers\n        self.action_pub = self.create_publisher(JointState, \'/vla/joint_commands\', 10)\n\n        # Control loop timer\n        self.timer = self.create_timer(1.0 / control_freq, self.control_loop)\n\n        self.get_logger().info(f\'VLA Control Node initialized on {self.device}\')\n\n    def image_callback(self, msg):\n        """Store latest camera image"""\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n\n    def joint_callback(self, msg):\n        """Store latest joint state"""\n        # Assume 7-DOF arm\n        if len(msg.position) >= 7 and len(msg.velocity) >= 7:\n            self.current_joints = {\n                \'position\': np.array(msg.position[:7]),\n                \'velocity\': np.array(msg.velocity[:7])\n            }\n\n    def task_callback(self, msg):\n        """Update current task description"""\n        self.current_task = msg.data\n        self.get_logger().info(f\'New task: {self.current_task}\')\n\n    def control_loop(self):\n        """Main control loop: run VLA model and publish actions"""\n\n        # Check if we have all required inputs\n        if self.current_image is None or self.current_joints is None:\n            return\n\n        try:\n            # Prepare inputs\n            image_tensor = self._preprocess_image(self.current_image)\n            proprio_tensor = self._preprocess_proprioception(self.current_joints)\n\n            # Run VLA model\n            actions = self.model.predict_action(\n                image_tensor,\n                [self.current_task],\n                proprio_tensor\n            )\n\n            # Publish actions\n            self._publish_actions(actions[0].cpu().numpy())\n\n        except Exception as e:\n            self.get_logger().error(f\'Control loop error: {str(e)}\')\n\n    def _preprocess_image(self, image_np):\n        """Convert numpy image to model input tensor"""\n        # Resize to 224x224 (CLIP standard)\n        image_resized = cv2.resize(image_np, (224, 224))\n\n        # Normalize to [0, 1] and convert to tensor\n        image_tensor = torch.from_numpy(image_resized).float() / 255.0\n\n        # (H, W, C) \u2192 (C, H, W)\n        image_tensor = image_tensor.permute(2, 0, 1)\n\n        # Add batch dimension\n        image_tensor = image_tensor.unsqueeze(0).to(self.device)\n\n        return image_tensor\n\n    def _preprocess_proprioception(self, joints):\n        """Convert joint state dict to tensor"""\n        # Concatenate position and velocity\n        proprio_np = np.concatenate([joints[\'position\'], joints[\'velocity\']])\n\n        # Normalize (simple scaling, real systems need careful calibration)\n        proprio_np = proprio_np / 3.0  # Rough normalization to [-1, 1]\n\n        proprio_tensor = torch.from_numpy(proprio_np).float().unsqueeze(0)\n        return proprio_tensor.to(self.device)\n\n    def _publish_actions(self, actions):\n        """Publish predicted actions as joint commands"""\n        msg = JointState()\n        msg.header.stamp = self.get_clock().now().to_msg()\n\n        # Scale actions from [-1, 1] to actual joint limits\n        # (In real system, use robot-specific scaling)\n        scaled_actions = actions * 0.1  # Conservative scaling\n\n        msg.velocity = scaled_actions.tolist()\n\n        self.action_pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAControlNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Architecture Points"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal fusion"}),": Vision (CLIP), language (CLIP text), proprioception (MLP)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discretized actions"}),": Actions quantized into 256 bins (reduces distribution complexity)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Frozen encoders"}),": CLIP weights frozen, only policy head trained (transfer learning)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time capable"}),": Forward pass ~50ms on GPU"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch VLA control"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 run my_robot_pkg vla_control_node --ros-args \\\n  -p model_path:=/path/to/vla_model.pth \\\n  -p control_frequency:=20.0\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-lab-exercise-3-vla-data-collection-and-fine-tuning",children:"\ud83d\udd2c Lab Exercise 3: VLA Data Collection and Fine-Tuning"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Collect demonstration data and fine-tune the VLA model."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Phase 1: Data Collection"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Teleoperate robot in simulation (Gazebo or Isaac)"}),"\n",(0,t.jsx)(n.li,{children:"Record episodes: (image, task_description, joint_state, action)"}),"\n",(0,t.jsxs)(n.li,{children:["Save dataset in HDF5 format:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'episode_0/\n  \u251c\u2500 observations/images: (T, 224, 224, 3)\n  \u251c\u2500 observations/proprio: (T, 14)\n  \u251c\u2500 actions: (T, 7)\n  \u2514\u2500 language: "pick up red block"\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Phase 2: Training"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Implement training loop:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"for batch in dataloader:\n    action_logits = model(batch['image'], batch['text'], batch['proprio'])\n    loss = F.cross_entropy(action_logits, batch['action_bins'])\n    loss.backward()\n    optimizer.step()\n"})}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Train for 50k steps with AdamW optimizer"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate on held-out tasks"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Metric"}),": Model achieves over 60% success rate on pick-and-place validation tasks."]}),"\n",(0,t.jsx)(n.h3,{id:"-assessment-checklist---week-3",children:"\u2705 Assessment Checklist - Week 3"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","VLA model forward pass completes in under 100ms"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multimodal fusion correctly combines vision, language, and proprioception"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Discretized action outputs map to valid continuous actions"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data collection pipeline records synchronized observations and actions"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fine-tuned model outperforms random baseline by more than 3x"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sim-to-real gap analysis completed (domain randomization helps?)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-module-summary",children:"\ud83d\udcdd Module Summary"}),"\n",(0,t.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision Foundation Models"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 CLIP enables zero-shot object detection via natural language"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Sliding window + NMS enables spatial localization"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Vision transformers outperform CNNs for open-vocabulary tasks"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLM Task Planning"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Prompt engineering is critical for constraining LLM outputs"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Few-shot examples dramatically improve plan quality"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Conversational error handling enables robust human-robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 LLMs excel at high-level planning but need grounding in perception"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VLA End-to-End Models"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 VLA models unify perception, planning, and control"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Multimodal fusion (vision + language + proprio) enables emergent reasoning"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Discretized actions simplify distribution modeling"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Transfer learning from vision-language models accelerates training"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"skills-mastered",children:"Skills Mastered"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrated CLIP for visual grounding in ROS 2"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implemented LLM-based task planners with structured outputs"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Built conversational command parsing with error recovery"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Designed and deployed multimodal VLA architectures"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Collected robot demonstration data for imitation learning"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fine-tuned VLA models on custom tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CLIP Limitations"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Poor at fine-grained spatial reasoning (exact pose estimation)"}),"\n",(0,t.jsx)(n.li,{children:"Struggles with occlusions and clutter"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Combine with traditional detection when precision needed"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLM Hallucinations"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"LLMs may generate invalid actions or assume capabilities robot doesn't have"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Strict output validation, constrain action vocabulary"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VLA Data Hunger"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"VLA models require thousands of demonstrations"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Use simulation for data generation, domain randomization"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sim-to-Real Gap"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Models trained in simulation may fail on real robots"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Visual domain randomization, sensor noise injection, real-world fine-tuning"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-final-project-multimodal-voice-controlled-robot",children:"\ud83c\udf93 Final Project: Multimodal Voice-Controlled Robot"}),"\n",(0,t.jsx)(n.h3,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Build an autonomous system that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accepts voice commands"})," (use Whisper for speech-to-text)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plans tasks with LLM"})," (GPT-4 task decomposition)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perceives environment"})," (CLIP visual grounding)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Executes actions"})," (VLA model or motion primitives)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Provides feedback"})," (text-to-speech status updates)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-workflow",children:"Example Workflow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Go to the kitchen and grab the red mug"\n  \u2193\n[Whisper] Transcribe audio \u2192 text\n  \u2193\n[LLM] Plan: [navigate(kitchen), detect(red mug), grasp(mug), navigate(user), place(table)]\n  \u2193\n[Navigation] Move to kitchen (Nav2 + Isaac Sim)\n  \u2193\n[CLIP] Detect "red mug" in camera feed \u2192 bounding box\n  \u2193\n[VLA Model] Execute grasp action from visual input\n  \u2193\n[Navigation] Return to user\n  \u2193\n[TTS] "Task complete, here\'s your red mug"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"evaluation-rubric",children:"Evaluation Rubric"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Criteria"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Voice Interface"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Transcription accuracy over 90%, handles background noise"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Task Planning"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"LLM generates valid, safe action sequences"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Visual Grounding"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"CLIP correctly identifies target objects over 80%"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Action Execution"})}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"Robot successfully completes pick-and-place"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Error Handling"})}),(0,t.jsx)(n.td,{children:"10%"}),(0,t.jsx)(n.td,{children:"Gracefully handles failures (object not found, etc.)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Integration"})}),(0,t.jsx)(n.td,{children:"10%"}),(0,t.jsx)(n.td,{children:"All components work together end-to-end"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"deliverables",children:"Deliverables"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code"}),": Complete ROS 2 workspace with all nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Video Demo"}),": 3-minute demonstration of successful task completion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Report"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System architecture diagram"}),"\n",(0,t.jsx)(n.li,{children:"Performance metrics (success rate, latency breakdown)"}),"\n",(0,t.jsx)(n.li,{children:"Challenges and solutions"}),"\n",(0,t.jsx)(n.li,{children:"Future improvements"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Presentation"}),": 10-minute demo + 5-minute Q&A"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,t.jsx)(n.h3,{id:"papers",children:"Papers"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RT-1"}),': "RT-1: Robotics Transformer for Real-World Control at Scale" (2022)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RT-2"}),': "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control" (2023)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLIP"}),': "Learning Transferable Visual Models From Natural Language Supervision" (2021)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PaLM-E"}),': "PaLM-E: An Embodied Multimodal Language Model" (2023)']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"code-repositories",children:"Code Repositories"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI CLIP"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/openai/CLIP",children:"https://github.com/openai/CLIP"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robotics Transformer (RT-1)"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/google-research/robotics_transformer",children:"https://github.com/google-research/robotics_transformer"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hugging Face Transformers"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/huggingface/transformers",children:"https://github.com/huggingface/transformers"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"datasets",children:"Datasets"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open X-Embodiment"}),": Large-scale robot manipulation dataset (800k+ trajectories)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RoboNet"}),": Multi-robot dataset for visual imitation learning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CALVIN"}),": Benchmark for language-conditioned manipulation"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tools",children:"Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Weights & Biases"}),": Experiment tracking for VLA training"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"TensorBoard"}),": Visualize training metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rerun"}),": Visualize multimodal robot data (images + poses + text)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"online-courses",children:"Online Courses"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stanford CS336"}),": Robot Learning (focuses on VLA models)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DeepMind x UCL"}),": Deep Learning Lecture on Multimodal Models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hugging Face Course"}),": Transformers and Vision-Language Models"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-integration-with-previous-modules",children:"\ud83d\udd17 Integration with Previous Modules"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Module 1 (ROS 2)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["CLIP node publishes ",(0,t.jsx)(n.code,{children:"Detection2DArray"})," messages"]}),"\n",(0,t.jsx)(n.li,{children:"LLM planner uses ROS 2 services for task requests"}),"\n",(0,t.jsx)(n.li,{children:"VLA model subscribes to joint states and camera feeds"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Module 2 (Simulation)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Gazebo/Unity provides synthetic data for VLA training"}),"\n",(0,t.jsx)(n.li,{children:"Domain randomization improves sim-to-real transfer"}),"\n",(0,t.jsx)(n.li,{children:"Photorealistic Unity rendering helps CLIP generalize"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Module 3 (NVIDIA Isaac)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Isaac ROS DOPE for 6D pose estimation (complements CLIP)"}),"\n",(0,t.jsx)(n.li,{children:"TensorRT optimization for VLA model inference"}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim synthetic data generation for VLA datasets"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Complete Tech Stack"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Voice Input (Whisper)\n    \u2193\nLLM Planning (GPT-4)\n    \u2193\nVisual Grounding (CLIP) \u2500\u2500\u2500\u2500\u2500\u2510\n    \u2193                        \u2502\nVLA Model (RT-1/RT-2) \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2193                        \u2502\nROS 2 Control (Module 1) \u2500\u2500\u2500\u2500\u2524\n    \u2193                        \u2502\nSimulation (Module 2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2193                        \u2502\nIsaac GPU Accel (Module 3) \u2500\u2500\u2500\u2518\n    \u2193\nPhysical Robot\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations!"})," You've completed the VLA module and now have the skills to build cognitive, language-grounded robots that understand and execute natural language commands. This represents the cutting edge of Physical AI research."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": Move on to the remaining chapters to explore real-world deployment, ethics, and advanced topics in humanoid robotics."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);