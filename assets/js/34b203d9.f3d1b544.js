"use strict";(globalThis.webpackChunkteaching_physical_ai_robotics_book=globalThis.webpackChunkteaching_physical_ai_robotics_book||[]).push([[28],{3180:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-04/module-ros2","title":"Module 1: The Robotic Nervous System (ROS 2 Humble)","description":"\ud83d\udcda Learning Path Overview","source":"@site/docs/chapter-04/module-ros2.md","sourceDirName":"chapter-04","slug":"/chapter-04/module-ros2","permalink":"/humanoid-robotics-book/docs/chapter-04/module-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/Aisha-Sarfaraz/humanoid-robotics-book/tree/main/docs/chapter-04/module-ros2.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Curriculum Design Framework","permalink":"/humanoid-robotics-book/docs/chapter-04/course-structure"},"next":{"title":"4.3 Module 2: The Digital Twin (Gazebo Classic 11 / Unity) - Weeks 6-7","permalink":"/humanoid-robotics-book/docs/chapter-04/module-simulation"}}');var r=i(4848),t=i(8453);const o={},l="Module 1: The Robotic Nervous System (ROS 2 Humble)",a={},c=[{value:"\ud83d\udcda Learning Path Overview",id:"-learning-path-overview",level:2},{value:"Week 1: ROS 2 Architecture &amp; Pub/Sub Pattern",id:"week-1-ros-2-architecture--pubsub-pattern",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory",level:3},{value:"\ud83d\udcbb Code Example 1: Temperature Sensor Publisher",id:"-code-example-1-temperature-sensor-publisher",level:3},{value:"\ud83d\udcbb Code Example 2: Temperature Monitor Subscriber",id:"-code-example-2-temperature-monitor-subscriber",level:3},{value:"\ud83d\ude80 Running the Example",id:"-running-the-example",level:3},{value:"\ud83d\udcca System Architecture Diagram",id:"-system-architecture-diagram",level:3},{value:"\ud83d\udd2c Lab Exercise 1: Build a Robot State Publisher",id:"-lab-exercise-1-build-a-robot-state-publisher",level:3},{value:"\u2705 Week 1 Assessment Checklist",id:"-week-1-assessment-checklist",level:3},{value:"Week 2: Bridging Python AI to ROS 2 Controllers",id:"week-2-bridging-python-ai-to-ros-2-controllers",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-1",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-1",level:3},{value:"\ud83d\udcbb Code Example 3: Voice-Controlled Robot",id:"-code-example-3-voice-controlled-robot",level:3},{value:"\ud83d\udcbb Code Example 4: ROS 2 Action Server",id:"-code-example-4-ros-2-action-server",level:3},{value:"\ud83d\udd2c Lab Exercise 2: AI-Powered Object Detection",id:"-lab-exercise-2-ai-powered-object-detection",level:3},{value:"\u2705 Week 2 Assessment Checklist",id:"-week-2-assessment-checklist",level:3},{value:"Week 3: URDF Robot Modeling &amp; Transforms",id:"week-3-urdf-robot-modeling--transforms",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-2",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-2",level:3},{value:"\ud83d\udcbb Code Example 5: Simple Humanoid URDF",id:"-code-example-5-simple-humanoid-urdf",level:3},{value:"\ud83d\udcbb Code Example 6: TF2 Transform Publisher",id:"-code-example-6-tf2-transform-publisher",level:3},{value:"\ud83d\udcbb Code Example 7: TF2 Listener (Get Camera Position)",id:"-code-example-7-tf2-listener-get-camera-position",level:3},{value:"\ud83d\ude80 Visualizing in RViz",id:"-visualizing-in-rviz",level:3},{value:"\ud83d\udd2c Lab Exercise 3: Full Humanoid Model",id:"-lab-exercise-3-full-humanoid-model",level:3},{value:"\u2705 Week 3 Assessment Checklist",id:"-week-3-assessment-checklist",level:3},{value:"\ud83d\udcdd Module Summary",id:"-module-summary",level:2},{value:"Key Concepts Mastered",id:"key-concepts-mastered",level:3},{value:"Real-World Applications",id:"real-world-applications",level:3},{value:"Common Pitfalls &amp; Solutions",id:"common-pitfalls--solutions",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"\ud83c\udf93 Final Project: Voice-Controlled Robot Arm",id:"-final-project-voice-controlled-robot-arm",level:2},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-1-the-robotic-nervous-system-ros-2-humble",children:"Module 1: The Robotic Nervous System (ROS 2 Humble)"})}),"\n",(0,r.jsx)(n.h2,{id:"-learning-path-overview",children:"\ud83d\udcda Learning Path Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration:"})," 3 weeks (9-12 hours/week)\n",(0,r.jsx)(n.strong,{children:"Difficulty:"})," Intermediate\n",(0,r.jsx)(n.strong,{children:"Prerequisites:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Python programming (functions, classes, async)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Linux command line basics"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Basic networking concepts"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning Progression:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Week 1: ROS 2 Basics] --\x3e B[Week 2: AI Integration]\n    B --\x3e C[Week 3: Robot Modeling]\n    C --\x3e D[Final Project]\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"By the end of this module:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Build distributed robotics applications with ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Integrate AI/ML models with robot controllers"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Model robots in URDF and manage transforms"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Debug and visualize robot systems"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-1-ros-2-architecture--pubsub-pattern",children:"Week 1: ROS 2 Architecture & Pub/Sub Pattern"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Understand ROS 2 nodes, topics, and messages"}),"\n",(0,r.jsx)(n.li,{children:"Implement publisher-subscriber communication"}),"\n",(0,r.jsx)(n.li,{children:"Use Quality of Service (QoS) profiles"}),"\n",(0,r.jsx)(n.li,{children:"Visualize system architecture with rqt_graph"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What is ROS 2?"}),"\nROS 2 is a middleware framework for building modular, distributed robot applications. Unlike monolithic programs, ROS 2 allows you to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run different parts of your robot on different computers"}),"\n",(0,r.jsx)(n.li,{children:"Restart individual components without stopping everything"}),"\n",(0,r.jsx)(n.li,{children:"Mix languages (Python + C++) seamlessly"}),"\n",(0,r.jsx)(n.li,{children:"Scale from hobby robots to production systems"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"The Pub/Sub Pattern:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Publisher Node          Topic          Subscriber Node\n   [Sensor] ---------\x3e /temp_data ---------\x3e [Monitor]\n                    (Float32 msg)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-1-temperature-sensor-publisher",children:"\ud83d\udcbb Code Example 1: Temperature Sensor Publisher"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: temp_publisher.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32\nimport random\n\nclass TempSensorNode(Node):\n    def __init__(self):\n        super().__init__('temperature_sensor')\n\n        # Create publisher on /temperature topic\n        self.publisher = self.create_publisher(Float32, 'temperature', 10)\n\n        # Publish every 1 second\n        self.timer = self.create_timer(1.0, self.publish_temperature)\n\n        self.get_logger().info('Temperature sensor started')\n\n    def publish_temperature(self):\n        msg = Float32()\n        # Simulate sensor reading (20-30\xb0C)\n        msg.data = 20.0 + random.uniform(0, 10)\n\n        self.publisher.publish(msg)\n        self.get_logger().info(f'Publishing: {msg.data:.2f}\xb0C')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TempSensorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-2-temperature-monitor-subscriber",children:"\ud83d\udcbb Code Example 2: Temperature Monitor Subscriber"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: temp_subscriber.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32\n\nclass TempMonitorNode(Node):\n    def __init__(self):\n        super().__init__('temperature_monitor')\n\n        # Subscribe to /temperature topic\n        self.subscription = self.create_subscription(\n            Float32,\n            'temperature',\n            self.temperature_callback,\n            10\n        )\n\n        self.get_logger().info('Temperature monitor started')\n\n    def temperature_callback(self, msg):\n        temp = msg.data\n\n        # Alert if temperature is too high\n        if temp > 28.0:\n            self.get_logger().warn(f'HIGH TEMP ALERT: {temp:.2f}\xb0C')\n        else:\n            self.get_logger().info(f'Temperature OK: {temp:.2f}\xb0C')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TempMonitorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-running-the-example",children:"\ud83d\ude80 Running the Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start publisher\ncd ~/ros2_ws\nsource install/setup.bash\npython3 temp_publisher.py\n\n# Terminal 2: Start subscriber\ncd ~/ros2_ws\nsource install/setup.bash\npython3 temp_subscriber.py\n\n# Terminal 3: Visualize graph\nrqt_graph\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-system-architecture-diagram",children:"\ud83d\udcca System Architecture Diagram"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TempSensorNode     \u2502\n\u2502  (Publisher)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 publishes\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 /temperature \u2502 (Topic)\n    \u2502  Float32     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 subscribes\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TempMonitorNode    \u2502\n\u2502  (Subscriber)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-1-build-a-robot-state-publisher",children:"\ud83d\udd2c Lab Exercise 1: Build a Robot State Publisher"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Create a two-node system:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"state_publisher.py"})," - Publishes robot battery level (0-100%)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"state_monitor.py"})," - Subscribes and warns when battery < 20%"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bonus Challenge:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Use custom message type ",(0,r.jsx)(n.code,{children:"BatteryStatus.msg"})," with fields: voltage, percentage, charging_status"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-1-assessment-checklist",children:"\u2705 Week 1 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create ROS 2 workspace with colcon"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Build publisher node that sends Float32 messages"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Build subscriber node that receives and processes messages"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use rqt_graph to visualize node connections"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement QoS profile (RELIABLE vs BEST_EFFORT)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-2-bridging-python-ai-to-ros-2-controllers",children:"Week 2: Bridging Python AI to ROS 2 Controllers"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-1",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate AI/ML models into ROS 2 nodes"}),"\n",(0,r.jsx)(n.li,{children:"Use ROS 2 Actions for goal-based behaviors"}),"\n",(0,r.jsx)(n.li,{children:"Implement service servers and clients"}),"\n",(0,r.jsx)(n.li,{children:"Connect OpenAI APIs with robot control"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-1",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why Separate AI from Control?"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Python    \u2502 Goals   \u2502     C++      \u2502\n\u2502 AI Planner  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502  Controller  \u2502\n\u2502 (Slow, ML)  \u2502         \u2502 (Fast, Real  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502   -time)     \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Actions:"})," Long-running tasks with feedback"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Goal:"}),' "Navigate to kitchen"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feedback:"})," Current position, % complete"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Result:"})," Success/Failure"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-3-voice-controlled-robot",children:"\ud83d\udcbb Code Example 3: Voice-Controlled Robot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: voice_robot_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport openai  # pip install openai\n\nclass VoiceRobotNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_robot\')\n\n        # Publisher for robot velocity commands\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Subscriber for voice transcriptions\n        self.voice_sub = self.create_subscription(\n            String,\n            \'/voice_input\',\n            self.voice_callback,\n            10\n        )\n\n        # OpenAI API key (set as environment variable)\n        openai.api_key = "your-api-key-here"\n\n        self.get_logger().info(\'Voice robot ready!\')\n\n    def voice_callback(self, msg):\n        command = msg.data.lower()\n        self.get_logger().info(f\'Heard: {command}\')\n\n        # Use GPT to interpret natural language\n        response = self.interpret_command(command)\n\n        # Execute motion based on interpretation\n        self.execute_motion(response)\n\n    def interpret_command(self, voice_text):\n        """Use GPT to convert voice to robot action"""\n        prompt = f"""\n        Convert this voice command to a robot action.\n        Command: "{voice_text}"\n\n        Respond with JSON:\n        {{"action": "move_forward|turn_left|turn_right|stop", "speed": 0.0-1.0}}\n        """\n\n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": prompt}]\n        )\n\n        import json\n        return json.loads(response.choices[0].message.content)\n\n    def execute_motion(self, action_dict):\n        """Send velocity commands to robot"""\n        twist = Twist()\n\n        action = action_dict.get(\'action\', \'stop\')\n        speed = action_dict.get(\'speed\', 0.5)\n\n        if action == \'move_forward\':\n            twist.linear.x = speed\n        elif action == \'turn_left\':\n            twist.angular.z = speed\n        elif action == \'turn_right\':\n            twist.angular.z = -speed\n        else:  # stop\n            twist.linear.x = 0.0\n            twist.angular.z = 0.0\n\n        self.cmd_vel_pub.publish(twist)\n        self.get_logger().info(f\'Executing: {action} at {speed}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceRobotNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-4-ros-2-action-server",children:"\ud83d\udcbb Code Example 4: ROS 2 Action Server"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: navigate_action_server.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer\nfrom nav_msgs.msg import Odometry\nfrom robot_interfaces.action import NavigateToPoint  # Custom action\nimport math\n\nclass NavigationActionServer(Node):\n    def __init__(self):\n        super().__init__('navigation_action_server')\n\n        self._action_server = ActionServer(\n            self,\n            NavigateToPoint,\n            'navigate_to_point',\n            self.execute_callback\n        )\n\n        self.current_x = 0.0\n        self.current_y = 0.0\n\n        self.get_logger().info('Navigation action server ready')\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute navigation to target point\"\"\"\n        self.get_logger().info('Executing navigation goal...')\n\n        target_x = goal_handle.request.target_x\n        target_y = goal_handle.request.target_y\n\n        feedback_msg = NavigateToPoint.Feedback()\n\n        # Simulate navigation with 10 steps\n        for step in range(10):\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return NavigateToPoint.Result(success=False)\n\n            # Update position (simulation)\n            self.current_x += (target_x - self.current_x) * 0.1\n            self.current_y += (target_y - self.current_y) * 0.1\n\n            # Calculate progress\n            distance = math.sqrt(\n                (target_x - self.current_x)**2 +\n                (target_y - self.current_y)**2\n            )\n\n            feedback_msg.current_x = self.current_x\n            feedback_msg.current_y = self.current_y\n            feedback_msg.distance_remaining = distance\n\n            goal_handle.publish_feedback(feedback_msg)\n            self.get_logger().info(f'Progress: {distance:.2f}m remaining')\n\n            rclpy.spin_once(self, timeout_sec=0.5)\n\n        goal_handle.succeed()\n        result = NavigateToPoint.Result()\n        result.success = True\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NavigationActionServer()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-2-ai-powered-object-detection",children:"\ud83d\udd2c Lab Exercise 2: AI-Powered Object Detection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Build a vision system that:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Captures camera images (simulated)"}),"\n",(0,r.jsx)(n.li,{children:"Uses YOLOv8 to detect objects"}),"\n",(0,r.jsxs)(n.li,{children:["Publishes detection results on ",(0,r.jsx)(n.code,{children:"/detections"})," topic"]}),"\n",(0,r.jsx)(n.li,{children:"Commands robot to approach detected objects"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Files to create:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"object_detector.py"})," - Vision + ML node"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"approach_controller.py"})," - Motion control node"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-2-assessment-checklist",children:"\u2705 Week 2 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create custom message/service definitions"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement action server with feedback"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate external AI API (OpenAI/HuggingFace)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handle async callbacks properly"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test with simulated robot in Gazebo"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-3-urdf-robot-modeling--transforms",children:"Week 3: URDF Robot Modeling & Transforms"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-2",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Model robots using URDF format"}),"\n",(0,r.jsx)(n.li,{children:"Define kinematic chains with joints"}),"\n",(0,r.jsx)(n.li,{children:"Use tf2 for coordinate transformations"}),"\n",(0,r.jsx)(n.li,{children:"Visualize and debug in RViz"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-2",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"URDF Structure:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<robot name="simple_humanoid">\n  <link name="base_link"/>         \x3c!-- Robot body --\x3e\n  <link name="right_arm"/>         \x3c!-- Arm link --\x3e\n\n  <joint name="shoulder" type="revolute">\n    <parent link="base_link"/>\n    <child link="right_arm"/>\n    <axis xyz="0 0 1"/>             \x3c!-- Rotation axis --\x3e\n    <limit lower="-1.57" upper="1.57"/>\n  </joint>\n</robot>\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Coordinate Frames:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"        world\n          \u2502\n          \u25bc\n      base_link \u2500\u2500\u2500\u2500> odom\n          \u2502\n          \u251c\u2500\u2500> left_arm\n          \u2502       \u2514\u2500\u2500> left_hand\n          \u2502\n          \u2514\u2500\u2500> right_arm\n                  \u2514\u2500\u2500> right_hand\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-5-simple-humanoid-urdf",children:"\ud83d\udcbb Code Example 5: Simple Humanoid URDF"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- File: simple_humanoid.urdf --\x3e\n<?xml version="1.0"?>\n<robot name="simple_humanoid">\n\n  \x3c!-- Base/Torso --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0.2 0.4 0.8 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10"/>\n      <inertia ixx="1.0" ixy="0.0" ixz="0.0"\n               iyy="1.0" iyz="0.0" izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Right Arm --\x3e\n  <link name="right_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.03"/>\n      </geometry>\n      <material name="red">\n        <color rgba="0.8 0.2 0.2 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.03"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1"/>\n      <inertia ixx="0.1" ixy="0.0" ixz="0.0"\n               iyy="0.1" iyz="0.0" izz="0.1"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Right Shoulder Joint --\x3e\n  <joint name="right_shoulder" type="revolute">\n    <parent link="base_link"/>\n    <child link="right_arm"/>\n    <origin xyz="0 -0.15 0.2" rpy="0 1.57 0"/>\n    <axis xyz="0 0 1"/>\n    <limit effort="100" lower="-2.0" upper="2.0" velocity="1.0"/>\n  </joint>\n\n  \x3c!-- Camera Sensor --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.03"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0.2 0.8 0.2 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Camera Joint (fixed to head) --\x3e\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.15 0 0.3" rpy="0 0 0"/>\n  </joint>\n\n</robot>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-6-tf2-transform-publisher",children:"\ud83d\udcbb Code Example 6: TF2 Transform Publisher"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: robot_tf_publisher.py\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport math\n\nclass RobotTFPublisher(Node):\n    def __init__(self):\n        super().__init__('robot_tf_publisher')\n\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Publish transforms at 50 Hz\n        self.timer = self.create_timer(0.02, self.publish_transforms)\n\n        self.shoulder_angle = 0.0  # Current joint angle\n\n        self.get_logger().info('TF Publisher started')\n\n    def publish_transforms(self):\n        # Animate shoulder joint\n        self.shoulder_angle += 0.01\n        if self.shoulder_angle > 2.0:\n            self.shoulder_angle = -2.0\n\n        # Publish base_link -> right_arm transform\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'base_link'\n        t.child_frame_id = 'right_arm'\n\n        # Position\n        t.transform.translation.x = 0.0\n        t.transform.translation.y = -0.15\n        t.transform.translation.z = 0.2\n\n        # Rotation (quaternion from shoulder angle)\n        t.transform.rotation.x = 0.0\n        t.transform.rotation.y = math.sin(self.shoulder_angle / 2)\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = math.cos(self.shoulder_angle / 2)\n\n        self.tf_broadcaster.sendTransform(t)\n\n        # Publish right_arm -> camera_link transform\n        t2 = TransformStamped()\n        t2.header.stamp = self.get_clock().now().to_msg()\n        t2.header.frame_id = 'right_arm'\n        t2.child_frame_id = 'camera_link'\n\n        t2.transform.translation.x = 0.4  # End of arm\n        t2.transform.translation.y = 0.0\n        t2.transform.translation.z = 0.0\n\n        t2.transform.rotation.x = 0.0\n        t2.transform.rotation.y = 0.0\n        t2.transform.rotation.z = 0.0\n        t2.transform.rotation.w = 1.0\n\n        self.tf_broadcaster.sendTransform(t2)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RobotTFPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-7-tf2-listener-get-camera-position",children:"\ud83d\udcbb Code Example 7: TF2 Listener (Get Camera Position)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: camera_position_listener.py\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformListener, Buffer\nfrom tf2_ros import LookupException, ConnectivityException, ExtrapolationException\n\nclass CameraPositionListener(Node):\n    def __init__(self):\n        super().__init__('camera_position_listener')\n\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Check camera position every 1 second\n        self.timer = self.create_timer(1.0, self.check_camera_position)\n\n        self.get_logger().info('Camera position listener started')\n\n    def check_camera_position(self):\n        try:\n            # Get transform from base_link to camera_link\n            trans = self.tf_buffer.lookup_transform(\n                'base_link',\n                'camera_link',\n                rclpy.time.Time(),\n                timeout=rclpy.duration.Duration(seconds=1.0)\n            )\n\n            x = trans.transform.translation.x\n            y = trans.transform.translation.y\n            z = trans.transform.translation.z\n\n            self.get_logger().info(\n                f'Camera position in base frame: '\n                f'x={x:.3f}, y={y:.3f}, z={z:.3f}'\n            )\n\n        except (LookupException, ConnectivityException, ExtrapolationException) as e:\n            self.get_logger().warn(f'TF lookup failed: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraPositionListener()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-visualizing-in-rviz",children:"\ud83d\ude80 Visualizing in RViz"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Start robot state publisher\nros2 run robot_state_publisher robot_state_publisher \\\n  --ros-args -p robot_description:="$(cat simple_humanoid.urdf)"\n\n# Terminal 2: Publish joint states\nros2 run joint_state_publisher_gui joint_state_publisher_gui\n\n# Terminal 3: Launch RViz\nrviz2\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"RViz Configuration:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Add \u2192 RobotModel"}),"\n",(0,r.jsx)(n.li,{children:"Add \u2192 TF (to see coordinate frames)"}),"\n",(0,r.jsx)(n.li,{children:'Fixed Frame \u2192 "base_link"'}),"\n",(0,r.jsx)(n.li,{children:"Adjust joint angles with GUI slider"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-3-full-humanoid-model",children:"\ud83d\udd2c Lab Exercise 3: Full Humanoid Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Extend the URDF to include:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Left arm with shoulder and elbow joints"}),"\n",(0,r.jsx)(n.li,{children:"Head with pan/tilt joints"}),"\n",(0,r.jsx)(n.li,{children:"IMU sensor on the torso"}),"\n",(0,r.jsx)(n.li,{children:"Both visual and collision geometry"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"URDF file with 5+ joints"}),"\n",(0,r.jsx)(n.li,{children:"Launch file that starts everything"}),"\n",(0,r.jsx)(n.li,{children:"RViz config showing all transforms"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-3-assessment-checklist",children:"\u2705 Week 3 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create complete URDF with 3+ joints"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Define visual and collision geometry"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Publish transforms with tf2_broadcaster"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Listen to transforms with tf2_listener"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualize robot in RViz with TF frames"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-module-summary",children:"\ud83d\udcdd Module Summary"}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-mastered",children:"Key Concepts Mastered"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS 2 Communication Patterns"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Publishers/Subscribers for streaming data"}),"\n",(0,r.jsx)(n.li,{children:"Services for request/response"}),"\n",(0,r.jsx)(n.li,{children:"Actions for long-running goals with feedback"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"AI-Robotics Integration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Separating planning (Python) from control (C++)"}),"\n",(0,r.jsx)(n.li,{children:"Using external APIs (OpenAI, Whisper) in nodes"}),"\n",(0,r.jsx)(n.li,{children:"Managing async execution"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Robot Modeling & Transforms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"URDF for kinematic structures"}),"\n",(0,r.jsx)(n.li,{children:"tf2 for coordinate frame management"}),"\n",(0,r.jsx)(n.li,{children:"Forward kinematics computation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manufacturing:"})," Coordinated robot arms with visual servoing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Service Robots:"})," Voice-controlled navigation and manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Research:"})," Multi-robot systems with distributed sensing"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-pitfalls--solutions",children:"Common Pitfalls & Solutions"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Problem"}),(0,r.jsx)(n.th,{children:"Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Messages not received"}),(0,r.jsx)(n.td,{children:"Check topic names match exactly"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"tf lookup fails"}),(0,r.jsx)(n.td,{children:"Ensure transforms published at >10 Hz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Robot model doesn't move in RViz"}),(0,r.jsx)(n.td,{children:"Publish joint_states topic"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"High CPU usage"}),(0,r.jsx)(n.td,{children:"Reduce publishing frequency"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Complete Module 2: Simulation (Gazebo, Isaac Sim)\n\u2705 Integrate perception pipelines\n\u2705 Deploy on real hardware"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-final-project-voice-controlled-robot-arm",children:"\ud83c\udf93 Final Project: Voice-Controlled Robot Arm"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective:"})," Combine all Week 1-3 skills into one system."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"URDF model of 3-DOF robot arm"}),"\n",(0,r.jsx)(n.li,{children:"Voice input \u2192 GPT \u2192 motion planning"}),"\n",(0,r.jsx)(n.li,{children:'Action server for "pick and place"'}),"\n",(0,r.jsx)(n.li,{children:"Visualize in RViz with live tf updates"}),"\n",(0,r.jsx)(n.li,{children:"Publish feedback: joint angles, end-effector pose"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Grading Rubric:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"URDF model quality (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Voice integration working (30%)"}),"\n",(0,r.jsx)(n.li,{children:"Smooth motion execution (25%)"}),"\n",(0,r.jsx)(n.li,{children:"Code quality and documentation (15%)"}),"\n",(0,r.jsx)(n.li,{children:"Demo video (10%)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Submission:"})," GitHub repo + 3-minute demo video"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Official Documentation:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Humble Docs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Tf2-Main.html",children:"tf2 Tutorials"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://wiki.ros.org/urdf/XML",children:"URDF Specification"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Video Tutorials:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://youtube.com/ros2tutorials",children:"ROS 2 Beginner Series"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://youtube.com/tf2ros",children:"TF2 Explained Visually"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Community:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://discourse.ros.org",children:"ROS Discourse"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://answers.ros.org",children:"ROS Answers"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://reddit.com/r/ROS",children:"r/ROS on Reddit"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Recommended Reading:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Programming Robots with ROS" by Quigley et al.'}),"\n",(0,r.jsx)(n.li,{children:'"A Gentle Introduction to ROS" by Jason M. O\'Kane'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);