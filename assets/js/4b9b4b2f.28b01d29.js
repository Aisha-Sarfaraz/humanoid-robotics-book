"use strict";(globalThis.webpackChunkteaching_physical_ai_robotics_book=globalThis.webpackChunkteaching_physical_ai_robotics_book||[]).push([[786],{4334:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-04/module-simulation","title":"Module 2: The Digital Twin (Gazebo & Unity Simulation)","description":"\ud83d\udcda Learning Path Overview","source":"@site/docs/chapter-04/module-simulation.md","sourceDirName":"chapter-04","slug":"/chapter-04/module-simulation","permalink":"/humanoid-robotics-book/docs/chapter-04/module-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Aisha-Sarfaraz/humanoid-robotics-book/tree/main/docs/chapter-04/module-simulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 1: The Robotic Nervous System (ROS 2 Humble)","permalink":"/humanoid-robotics-book/docs/chapter-04/module-ros2"},"next":{"title":"4.4 Module 3: The AI-Robot Brain (NVIDIA Isaac Platform) - Weeks 8-10","permalink":"/humanoid-robotics-book/docs/chapter-04/module-isaac"}}');var r=i(4848),t=i(8453);const a={},o="Module 2: The Digital Twin (Gazebo & Unity Simulation)",l={},c=[{value:"\ud83d\udcda Learning Path Overview",id:"-learning-path-overview",level:2},{value:"Week 1: Gazebo Physics Engine &amp; World Building",id:"week-1-gazebo-physics-engine--world-building",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory",level:3},{value:"\ud83d\udcbb Code Example 1: Simple Gazebo World",id:"-code-example-1-simple-gazebo-world",level:3},{value:"\ud83d\udcbb Code Example 2: Gazebo Launch File (ROS 2)",id:"-code-example-2-gazebo-launch-file-ros-2",level:3},{value:"\ud83d\udcbb Code Example 3: Apply Forces to Robot",id:"-code-example-3-apply-forces-to-robot",level:3},{value:"\ud83d\udcca Physics Simulation Architecture",id:"-physics-simulation-architecture",level:3},{value:"\ud83d\udd2c Lab Exercise 1: Obstacle Course Navigation",id:"-lab-exercise-1-obstacle-course-navigation",level:3},{value:"\u2705 Week 1 Assessment Checklist",id:"-week-1-assessment-checklist",level:3},{value:"Week 2: Sensor Simulation (LiDAR, Camera, IMU)",id:"week-2-sensor-simulation-lidar-camera-imu",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-1",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-1",level:3},{value:"\ud83d\udcbb Code Example 4: 2D LiDAR Sensor Plugin",id:"-code-example-4-2d-lidar-sensor-plugin",level:3},{value:"\ud83d\udcbb Code Example 5: RGB-D Camera Plugin",id:"-code-example-5-rgb-d-camera-plugin",level:3},{value:"\ud83d\udcbb Code Example 6: LiDAR Obstacle Detection",id:"-code-example-6-lidar-obstacle-detection",level:3},{value:"\ud83d\udcbb Code Example 7: IMU Sensor Fusion",id:"-code-example-7-imu-sensor-fusion",level:3},{value:"\ud83d\udd2c Lab Exercise 2: Sensor-Based Wall Following",id:"-lab-exercise-2-sensor-based-wall-following",level:3},{value:"\u2705 Week 2 Assessment Checklist",id:"-week-2-assessment-checklist",level:3},{value:"Week 3: Unity Integration for Photorealistic Rendering",id:"week-3-unity-integration-for-photorealistic-rendering",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives-2",level:3},{value:"\ud83d\udcd6 Theory",id:"-theory-2",level:3},{value:"\ud83d\udcbb Code Example 8: Unity ROS Connection",id:"-code-example-8-unity-ros-connection",level:3},{value:"\ud83d\udcbb Code Example 9: Unity Camera Publisher",id:"-code-example-9-unity-camera-publisher",level:3},{value:"\ud83d\udcbb Code Example 10: Synthetic Data Generator",id:"-code-example-10-synthetic-data-generator",level:3},{value:"\ud83d\udd2c Lab Exercise 3: ML Dataset Creation",id:"-lab-exercise-3-ml-dataset-creation",level:3},{value:"\u2705 Week 3 Assessment Checklist",id:"-week-3-assessment-checklist",level:3},{value:"\ud83d\udcdd Module Summary",id:"-module-summary",level:2},{value:"Key Concepts Mastered",id:"key-concepts-mastered",level:3},{value:"Real-World Applications",id:"real-world-applications",level:3},{value:"Common Pitfalls &amp; Solutions",id:"common-pitfalls--solutions",level:3},{value:"Simulation Best Practices",id:"simulation-best-practices",level:3},{value:"Next Steps",id:"next-steps",level:3},{value:"\ud83c\udf93 Final Project: Multi-Environment Testing Suite",id:"-final-project-multi-environment-testing-suite",level:2},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-2-the-digital-twin-gazebo--unity-simulation",children:"Module 2: The Digital Twin (Gazebo & Unity Simulation)"})}),"\n",(0,r.jsx)(n.h2,{id:"-learning-path-overview",children:"\ud83d\udcda Learning Path Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration:"})," 3 weeks (10-14 hours/week)\n",(0,r.jsx)(n.strong,{children:"Difficulty:"})," Intermediate to Advanced\n",(0,r.jsx)(n.strong,{children:"Prerequisites:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Completed Module 1 (ROS 2)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Basic physics (kinematics, dynamics)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 3D coordinate systems understanding"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning Progression:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Week 1: Gazebo Physics] --\x3e B[Week 2: Sensor Simulation]\n    B --\x3e C[Week 3: Unity Integration]\n    C --\x3e D[Hybrid Sim Project]\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"By the end of this module:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Simulate robots with realistic physics (gravity, friction, collisions)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Model sensors: LiDAR, cameras, IMUs, depth sensors"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Build photorealistic environments in Unity"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Test robot algorithms in safe virtual worlds"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-1-gazebo-physics-engine--world-building",children:"Week 1: Gazebo Physics Engine & World Building"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Understand rigid body physics and collision detection"}),"\n",(0,r.jsx)(n.li,{children:"Build simulation worlds with terrains and obstacles"}),"\n",(0,r.jsx)(n.li,{children:"Apply forces, torques, and gravity to robots"}),"\n",(0,r.jsx)(n.li,{children:"Debug physics issues using Gazebo GUI"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why Simulate?"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Real Robot Testing          Simulation Testing\n\u251c\u2500 Expensive ($$$)          \u251c\u2500 Free\n\u251c\u2500 Slow iterations          \u251c\u2500 Instant reset\n\u251c\u2500 Risk of damage           \u251c\u2500 Safe failure\n\u251c\u2500 Limited environments     \u251c\u2500 Any scenario\n\u2514\u2500 Hard to debug            \u2514\u2500 Full observability\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Physics Engine Components:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rigid Body Dynamics:"})," F = ma, torque = I\xd7\u03b1"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Collision Detection:"})," Mesh vs primitive shapes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact Forces:"})," Normal forces, friction cones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Constraints:"})," Joints, springs, dampers"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-1-simple-gazebo-world",children:"\ud83d\udcbb Code Example 1: Simple Gazebo World"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- File: robot_world.world --\x3e\n<?xml version="1.0"?>\n<sdf version="1.6">\n  <world name="robot_arena">\n\n    \x3c!-- Physics engine settings --\x3e\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n      <gravity>0 0 -9.81</gravity>\n    </physics>\n\n    \x3c!-- Sun lighting --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    \x3c!-- Ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    \x3c!-- Custom obstacle: Box --\x3e\n    <model name="obstacle_box">\n      <static>true</static>\n      <pose>2 0 0.5 0 0 0</pose>\n      <link name="box_link">\n        <collision name="box_collision">\n          <geometry>\n            <box>\n              <size>1 1 1</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="box_visual">\n          <geometry>\n            <box>\n              <size>1 1 1</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>1 0 0 1</ambient>\n            <diffuse>1 0 0 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n    \x3c!-- Ramp for testing --\x3e\n    <model name="ramp">\n      <static>true</static>\n      <pose>-3 0 0 0 0.3 0</pose>\n      <link name="ramp_link">\n        <collision name="ramp_collision">\n          <geometry>\n            <box>\n              <size>2 2 0.1</size>\n            </box>\n          </geometry>\n          <surface>\n            <friction>\n              <ode>\n                <mu>0.8</mu>\n                <mu2>0.8</mu2>\n              </ode>\n            </friction>\n          </surface>\n        </collision>\n        <visual name="ramp_visual">\n          <geometry>\n            <box>\n              <size>2 2 0.1</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0 1 0 1</ambient>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n  </world>\n</sdf>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-2-gazebo-launch-file-ros-2",children:"\ud83d\udcbb Code Example 2: Gazebo Launch File (ROS 2)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: gazebo_world_launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    pkg_share = get_package_share_directory('my_robot_sim')\n    world_file = os.path.join(pkg_share, 'worlds', 'robot_world.world')\n    urdf_file = os.path.join(pkg_share, 'urdf', 'robot.urdf')\n\n    # Declare arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Launch Gazebo with custom world\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            os.path.join(get_package_share_directory('gazebo_ros'),\n                        'launch', 'gazebo.launch.py')\n        ]),\n        launch_arguments={\n            'world': world_file,\n            'verbose': 'true',\n            'pause': 'false'\n        }.items()\n    )\n\n    # Spawn robot model\n    spawn_robot = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-entity', 'my_robot',\n            '-file', urdf_file,\n            '-x', '0', '-y', '0', '-z', '0.5',\n            '-robot_namespace', 'robot1'\n        ],\n        output='screen'\n    )\n\n    # Robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'robot_description': open(urdf_file).read()\n        }]\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument('use_sim_time', default_value='true'),\n        gazebo,\n        spawn_robot,\n        robot_state_publisher\n    ])\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-3-apply-forces-to-robot",children:"\ud83d\udcbb Code Example 3: Apply Forces to Robot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: force_controller.py\nimport rclpy\nfrom rclpy.node import Node\nfrom gazebo_msgs.srv import ApplyBodyWrench\nfrom geometry_msgs.msg import Wrench, Point, Vector3\nfrom std_msgs.msg import String\n\nclass ForceController(Node):\n    def __init__(self):\n        super().__init__('force_controller')\n\n        # Service client to apply forces in Gazebo\n        self.force_client = self.create_client(\n            ApplyBodyWrench,\n            '/gazebo/apply_body_wrench'\n        )\n\n        while not self.force_client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('Waiting for Gazebo service...')\n\n        # Subscribe to commands\n        self.cmd_sub = self.create_subscription(\n            String,\n            '/robot/push_command',\n            self.command_callback,\n            10\n        )\n\n        self.get_logger().info('Force controller ready')\n\n    def command_callback(self, msg):\n        \"\"\"Apply force based on command\"\"\"\n        command = msg.data.lower()\n\n        force = Vector3()\n        if command == 'forward':\n            force.x = 10.0\n        elif command == 'backward':\n            force.x = -10.0\n        elif command == 'left':\n            force.y = 10.0\n        elif command == 'right':\n            force.y = -10.0\n        elif command == 'up':\n            force.z = 20.0\n        else:\n            self.get_logger().warn(f'Unknown command: {command}')\n            return\n\n        self.apply_force(force)\n\n    def apply_force(self, force_vector):\n        \"\"\"Apply a force to the robot\"\"\"\n        request = ApplyBodyWrench.Request()\n\n        request.body_name = 'my_robot::base_link'\n\n        # Define wrench (force + torque)\n        wrench = Wrench()\n        wrench.force = force_vector\n        wrench.torque = Vector3()  # No torque\n\n        request.wrench = wrench\n        request.reference_point = Point()  # Center of mass\n        request.duration.sec = 1  # 1 second duration\n\n        # Send request\n        future = self.force_client.call_async(request)\n        future.add_done_callback(self.force_callback)\n\n    def force_callback(self, future):\n        try:\n            response = future.result()\n            if response.success:\n                self.get_logger().info('Force applied successfully!')\n            else:\n                self.get_logger().warn(f'Failed: {response.status_message}')\n        except Exception as e:\n            self.get_logger().error(f'Service call failed: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ForceController()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-physics-simulation-architecture",children:"\ud83d\udcca Physics Simulation Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Gazebo Simulation             \u2502\n\u2502                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Physics  \u2502\u2500\u2500\u2500>\u2502 Rigid Body   \u2502 \u2502\n\u2502  \u2502  Engine   \u2502    \u2502  Dynamics    \u2502 \u2502\n\u2502  \u2502  (ODE)    \u2502    \u2502  F=ma        \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502         \u2502                           \u2502\n\u2502         \u251c\u2500\u2500> Collision Detection    \u2502\n\u2502         \u251c\u2500\u2500> Contact Forces         \u2502\n\u2502         \u251c\u2500\u2500> Joint Constraints      \u2502\n\u2502         \u2514\u2500\u2500> Friction Models        \u2502\n\u2502                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502   Sensors  (LiDAR, Camera)    \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n   ROS 2 Topics (/scan, /camera, /odom)\n         \u2502\n         \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  Controller  \u2502\n   \u2502    Nodes     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-1-obstacle-course-navigation",children:"\ud83d\udd2c Lab Exercise 1: Obstacle Course Navigation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Create a Gazebo world with:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Start zone and goal zone"}),"\n",(0,r.jsx)(n.li,{children:"5+ obstacles (boxes, cylinders, ramps)"}),"\n",(0,r.jsx)(n.li,{children:"A mobile robot with differential drive"}),"\n",(0,r.jsx)(n.li,{children:"Python node that navigates autonomously"}),"\n",(0,r.jsx)(n.li,{children:"Emergency stop on collision detection"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bonus Challenge:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add moving obstacles"}),"\n",(0,r.jsx)(n.li,{children:"Implement path planning (A* or RRT)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"obstacle_course.world"})," file"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"navigator.py"})," node"]}),"\n",(0,r.jsx)(n.li,{children:"Video showing successful navigation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-1-assessment-checklist",children:"\u2705 Week 1 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Build custom Gazebo world with 5+ objects"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Spawn robot programmatically via ROS 2"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Apply forces/torques using Gazebo services"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Adjust physics parameters (gravity, friction, mass)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use Gazebo GUI to inspect collisions and forces"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-2-sensor-simulation-lidar-camera-imu",children:"Week 2: Sensor Simulation (LiDAR, Camera, IMU)"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-1",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Add sensor plugins to robot URDF/SDF"}),"\n",(0,r.jsx)(n.li,{children:"Process simulated sensor data in ROS 2"}),"\n",(0,r.jsx)(n.li,{children:"Understand sensor noise models"}),"\n",(0,r.jsx)(n.li,{children:"Implement sensor fusion algorithms"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-1",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sensor Types & Applications:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sensor     \u2502   Output      \u2502   Use Case      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2D LiDAR     \u2502 LaserScan     \u2502 2D Mapping      \u2502\n\u2502 3D LiDAR     \u2502 PointCloud    \u2502 3D Perception   \u2502\n\u2502 RGB Camera   \u2502 Image         \u2502 Object Detect   \u2502\n\u2502 Depth Camera \u2502 DepthImage    \u2502 3D Vision       \u2502\n\u2502 IMU          \u2502 Accel + Gyro  \u2502 Orientation     \u2502\n\u2502 GPS          \u2502 NavSatFix     \u2502 Localization    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sensor Noise Models:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gaussian Noise:"})," N(\u03bc=0, \u03c3\xb2) - Most common"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bias Drift:"})," Slow accumulation over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quantization:"})," Discrete sensor resolution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Outliers:"})," Random spurious readings"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-4-2d-lidar-sensor-plugin",children:"\ud83d\udcbb Code Example 4: 2D LiDAR Sensor Plugin"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add to robot URDF --\x3e\n<gazebo reference="lidar_link">\n  <sensor name="lidar_sensor" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.12</min>\n        <max>10.0</max>\n        <resolution>0.01</resolution>\n      </range>\n\n      \x3c!-- Realistic noise model --\x3e\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </ray>\n\n    \x3c!-- ROS 2 plugin --\x3e\n    <plugin name="gazebo_ros_laser" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/robot</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>lidar_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-5-rgb-d-camera-plugin",children:"\ud83d\udcbb Code Example 5: RGB-D Camera Plugin"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="rgbd_camera" type="depth">\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n\n      \x3c!-- Lens distortion model --\x3e\n      <distortion>\n        <k1>-0.1</k1>\n        <k2>0.05</k2>\n        <k3>0.0</k3>\n        <p1>0.0</p1>\n        <p2>0.0</p2>\n        <center>0.5 0.5</center>\n      </distortion>\n\n      \x3c!-- Image noise --\x3e\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n\n    \x3c!-- ROS 2 Camera Plugin --\x3e\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/robot</namespace>\n        <remapping>image_raw:=camera/image</remapping>\n        <remapping>depth/image_raw:=camera/depth</remapping>\n        <remapping>camera_info:=camera/camera_info</remapping>\n      </ros>\n      <frame_name>camera_link</frame_name>\n    </plugin>\n\n    <update_rate>30</update_rate>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-6-lidar-obstacle-detection",children:"\ud83d\udcbb Code Example 6: LiDAR Obstacle Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: lidar_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import Bool\nimport numpy as np\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            '/robot/scan',\n            self.scan_callback,\n            10\n        )\n\n        # Publish collision warning\n        self.collision_pub = self.create_publisher(Bool, '/collision_warning', 10)\n\n        # Parameters\n        self.safe_distance = 0.5  # meters\n        self.obstacle_threshold = 1.0  # meters\n\n        self.get_logger().info('LiDAR processor started')\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan for obstacle detection\"\"\"\n        ranges = np.array(msg.ranges)\n\n        # Filter invalid readings\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        if len(valid_ranges) == 0:\n            return\n\n        # Find closest obstacle\n        min_distance = np.min(valid_ranges)\n        min_index = np.argmin(ranges)\n\n        # Calculate angle\n        angle = msg.angle_min + (min_index * msg.angle_increment)\n        angle_deg = np.degrees(angle)\n\n        # Collision warning\n        collision_warning = Bool()\n        if min_distance < self.safe_distance:\n            self.get_logger().warn(\n                f'COLLISION WARNING! Obstacle at {min_distance:.2f}m, '\n                f'{angle_deg:.1f}\xb0'\n            )\n            collision_warning.data = True\n        else:\n            collision_warning.data = False\n\n        self.collision_pub.publish(collision_warning)\n\n        # Find navigable gaps\n        gaps = self.find_navigable_gaps(ranges, msg)\n\n        if gaps:\n            best_gap = max(gaps, key=lambda g: g['width'])\n            self.get_logger().info(\n                f'Best gap at {np.degrees(best_gap[\"angle\"]):.1f}\xb0 '\n                f'(width: {np.degrees(best_gap[\"width\"]):.1f}\xb0)'\n            )\n\n    def find_navigable_gaps(self, ranges, scan_msg, threshold=1.0):\n        \"\"\"Identify open spaces for navigation\"\"\"\n        gaps = []\n        in_gap = False\n        gap_start = 0\n\n        for i, r in enumerate(ranges):\n            # Check if reading is valid and beyond threshold\n            is_open = r > threshold and not np.isinf(r)\n\n            if is_open and not in_gap:\n                gap_start = i\n                in_gap = True\n            elif not is_open and in_gap:\n                gap_end = i - 1\n                gap_center_idx = (gap_start + gap_end) / 2\n                gap_angle = scan_msg.angle_min + \\\n                           (gap_center_idx * scan_msg.angle_increment)\n                gap_width = (gap_end - gap_start) * scan_msg.angle_increment\n\n                gaps.append({\n                    'angle': gap_angle,\n                    'width': gap_width,\n                    'start_idx': gap_start,\n                    'end_idx': gap_end\n                })\n                in_gap = False\n\n        return gaps\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LidarProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-7-imu-sensor-fusion",children:"\ud83d\udcbb Code Example 7: IMU Sensor Fusion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: imu_fusion.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3Stamped\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\nclass IMUFusion(Node):\n    def __init__(self):\n        super().__init__('imu_fusion')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/robot/imu',\n            self.imu_callback,\n            10\n        )\n\n        # Publish fused orientation\n        self.orientation_pub = self.create_publisher(\n            Vector3Stamped,\n            '/robot/orientation',\n            10\n        )\n\n        # Complementary filter state\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.yaw = 0.0\n\n        # Filter coefficient (0.98 = trust gyro more)\n        self.alpha = 0.98\n\n        self.last_time = None\n\n        self.get_logger().info('IMU fusion node started')\n\n    def imu_callback(self, msg):\n        \"\"\"Apply complementary filter for sensor fusion\"\"\"\n        current_time = self.get_clock().now()\n\n        if self.last_time is None:\n            self.last_time = current_time\n            return\n\n        # Calculate dt\n        dt = (current_time - self.last_time).nanoseconds / 1e9\n        self.last_time = current_time\n\n        # === Gyroscope Integration ===\n        gyro_x = msg.angular_velocity.x\n        gyro_y = msg.angular_velocity.y\n        gyro_z = msg.angular_velocity.z\n\n        gyro_roll = self.roll + gyro_x * dt\n        gyro_pitch = self.pitch + gyro_y * dt\n        gyro_yaw = self.yaw + gyro_z * dt\n\n        # === Accelerometer Angles ===\n        accel_x = msg.linear_acceleration.x\n        accel_y = msg.linear_acceleration.y\n        accel_z = msg.linear_acceleration.z\n\n        # Calculate roll and pitch from accelerometer\n        accel_roll = np.arctan2(accel_y, accel_z)\n        accel_pitch = np.arctan2(\n            -accel_x,\n            np.sqrt(accel_y**2 + accel_z**2)\n        )\n\n        # === Complementary Filter Fusion ===\n        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll\n        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch\n        self.yaw = gyro_yaw  # No magnetometer, so just integrate gyro\n\n        # Publish fused orientation\n        orientation_msg = Vector3Stamped()\n        orientation_msg.header.stamp = current_time.to_msg()\n        orientation_msg.header.frame_id = 'imu_link'\n        orientation_msg.vector.x = self.roll\n        orientation_msg.vector.y = self.pitch\n        orientation_msg.vector.z = self.yaw\n\n        self.orientation_pub.publish(orientation_msg)\n\n        self.get_logger().info(\n            f'Orientation (deg) - '\n            f'Roll: {np.degrees(self.roll):.1f}, '\n            f'Pitch: {np.degrees(self.pitch):.1f}, '\n            f'Yaw: {np.degrees(self.yaw):.1f}'\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IMUFusion()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-2-sensor-based-wall-following",children:"\ud83d\udd2c Lab Exercise 2: Sensor-Based Wall Following"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Implement a wall-following robot using LiDAR"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Keep 0.5m distance from right wall"}),"\n",(0,r.jsx)(n.li,{children:"Handle corners smoothly"}),"\n",(0,r.jsx)(n.li,{children:"Detect and avoid obstacles"}),"\n",(0,r.jsx)(n.li,{children:"Work in any maze-like environment"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Algorithm Structure:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def wall_follow_control(scan_data):\n    right_distance = get_right_wall_distance(scan_data)\n\n    if right_distance > 0.6:\n        turn_right()\n    elif right_distance < 0.4:\n        turn_left()\n    else:\n        go_straight()\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"wall_follower.py"})," node"]}),"\n",(0,r.jsx)(n.li,{children:"Test in Gazebo maze world"}),"\n",(0,r.jsx)(n.li,{children:"Video demonstration"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-2-assessment-checklist",children:"\u2705 Week 2 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add 3+ sensor types to robot URDF"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Process LiDAR scans for obstacle detection"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement complementary filter for IMU"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualize camera feed in RViz"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configure realistic sensor noise parameters"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"week-3-unity-integration-for-photorealistic-rendering",children:"Week 3: Unity Integration for Photorealistic Rendering"}),"\n",(0,r.jsx)(n.h3,{id:"-learning-objectives-2",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up Unity with ROS 2 bridge (ROS-TCP-Connector)"}),"\n",(0,r.jsx)(n.li,{children:"Create high-fidelity visual environments"}),"\n",(0,r.jsx)(n.li,{children:"Generate synthetic training data for ML models"}),"\n",(0,r.jsx)(n.li,{children:"Compare Gazebo vs Unity trade-offs"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-theory-2",children:"\ud83d\udcd6 Theory"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Gazebo vs Unity Comparison:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Feature   \u2502     Gazebo      \u2502      Unity      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Graphics   \u2502 Basic/Moderate  \u2502 Photorealistic  \u2502\n\u2502 Physics    \u2502 Very Accurate   \u2502 Good (PhysX)    \u2502\n\u2502 ML Dataset \u2502 Limited         \u2502 Excellent       \u2502\n\u2502 Setup      \u2502 ROS Native      \u2502 Needs Bridge    \u2502\n\u2502 Speed      \u2502 Real-time       \u2502 GPU-dependent   \u2502\n\u2502 Best For   \u2502 Physics testing \u2502 Vision/ML data  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use Cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo:"})," Control validation, physics simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity:"})," Computer vision, synthetic datasets, demos, VR"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-8-unity-ros-connection",children:"\ud83d\udcbb Code Example 8: Unity ROS Connection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'// File: ROSConnectionManager.cs (Unity C#)\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Std;\n\npublic class ROSConnectionManager : MonoBehaviour\n{\n    [Header("ROS Settings")]\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n\n    [Header("Robot Control")]\n    public Rigidbody robotRigidbody;\n    public float speedMultiplier = 1.0f;\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        // Connect to ROS 2\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Connect(rosIPAddress, rosPort);\n\n        Debug.Log($"Connected to ROS at {rosIPAddress}:{rosPort}");\n\n        // Subscribe to velocity commands\n        ros.Subscribe<TwistMsg>("/cmd_vel", ApplyVelocityCommand);\n\n        // Publish robot status\n        InvokeRepeating("PublishStatus", 1.0f, 0.1f);\n    }\n\n    void ApplyVelocityCommand(TwistMsg twist)\n    {\n        if (robotRigidbody == null) return;\n\n        // Extract linear and angular velocities\n        float linearX = (float)twist.linear.x * speedMultiplier;\n        float angularZ = (float)twist.angular.z * speedMultiplier;\n\n        // Apply to rigidbody\n        Vector3 velocity = transform.forward * linearX;\n        robotRigidbody.velocity = velocity;\n\n        Vector3 angularVel = Vector3.up * angularZ;\n        robotRigidbody.angularVelocity = angularVel;\n\n        Debug.Log($"Cmd: Linear={linearX:F2}, Angular={angularZ:F2}");\n    }\n\n    void PublishStatus()\n    {\n        // Publish heartbeat\n        StringMsg status = new StringMsg("Unity robot active");\n        ros.Publish("/robot/status", status);\n    }\n\n    void OnApplicationQuit()\n    {\n        ros.Disconnect();\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-9-unity-camera-publisher",children:"\ud83d\udcbb Code Example 9: Unity Camera Publisher"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'// File: CameraImagePublisher.cs\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Std;\n\npublic class CameraImagePublisher : MonoBehaviour\n{\n    [Header("Camera Settings")]\n    public Camera robotCamera;\n    public string topicName = "/robot/camera/image";\n    public int publishRateHz = 10;\n\n    [Header("Image Settings")]\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n\n    private ROSConnection ros;\n    private float timer;\n    private float publishInterval;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<ImageMsg>(topicName);\n\n        publishInterval = 1.0f / publishRateHz;\n\n        if (robotCamera == null)\n        {\n            robotCamera = GetComponent<Camera>();\n        }\n\n        Debug.Log($"Camera publisher started on {topicName}");\n    }\n\n    void Update()\n    {\n        timer += Time.deltaTime;\n\n        if (timer >= publishInterval)\n        {\n            PublishCameraImage();\n            timer = 0;\n        }\n    }\n\n    void PublishCameraImage()\n    {\n        // Create render texture\n        RenderTexture rt = new RenderTexture(\n            imageWidth, imageHeight, 24, RenderTextureFormat.ARGB32\n        );\n\n        robotCamera.targetTexture = rt;\n        robotCamera.Render();\n\n        // Read pixels\n        RenderTexture.active = rt;\n        Texture2D image = new Texture2D(\n            imageWidth, imageHeight, TextureFormat.RGB24, false\n        );\n        image.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        image.Apply();\n\n        // Create ROS message\n        ImageMsg msg = new ImageMsg();\n\n        // Header\n        HeaderMsg header = new HeaderMsg();\n        header.stamp = GetROSTimestamp();\n        header.frame_id = "camera_link";\n        msg.header = header;\n\n        // Image data\n        msg.height = (uint)imageHeight;\n        msg.width = (uint)imageWidth;\n        msg.encoding = "rgb8";\n        msg.is_bigendian = 0;\n        msg.step = (uint)(imageWidth * 3);\n        msg.data = image.GetRawTextureData();\n\n        // Publish\n        ros.Publish(topicName, msg);\n\n        // Cleanup\n        robotCamera.targetTexture = null;\n        RenderTexture.active = null;\n        Destroy(rt);\n        Destroy(image);\n    }\n\n    RosMessageTypes.BuiltinInterfaces.TimeMsg GetROSTimestamp()\n    {\n        double unixTime = (System.DateTime.UtcNow -\n                          new System.DateTime(1970, 1, 1)).TotalSeconds;\n\n        return new RosMessageTypes.BuiltinInterfaces.TimeMsg\n        {\n            sec = (int)unixTime,\n            nanosec = (uint)((unixTime % 1) * 1e9)\n        };\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-code-example-10-synthetic-data-generator",children:"\ud83d\udcbb Code Example 10: Synthetic Data Generator"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'// File: SyntheticDataGenerator.cs\nusing UnityEngine;\nusing System.IO;\nusing System.Collections.Generic;\n\npublic class SyntheticDataGenerator : MonoBehaviour\n{\n    [Header("Dataset Settings")]\n    public string datasetPath = "D:/Datasets/RobotVision";\n    public int imagesToGenerate = 1000;\n    public Camera captureCamera;\n\n    [Header("Randomization")]\n    public GameObject[] objectPrefabs;\n    public Material[] skyboxMaterials;\n    public Light sunLight;\n\n    private int capturedCount = 0;\n\n    public void GenerateDataset()\n    {\n        StartCoroutine(GenerateDatasetCoroutine());\n    }\n\n    System.Collections.IEnumerator GenerateDatasetCoroutine()\n    {\n        // Create directories\n        Directory.CreateDirectory(Path.Combine(datasetPath, "images"));\n        Directory.CreateDirectory(Path.Combine(datasetPath, "labels"));\n\n        for (int i = 0; i < imagesToGenerate; i++)\n        {\n            // Randomize scene\n            RandomizeScene();\n\n            // Wait for physics to settle\n            yield return new WaitForSeconds(0.1f);\n\n            // Capture image and labels\n            CaptureFrame(i);\n\n            capturedCount++;\n            Debug.Log($"Captured {capturedCount}/{imagesToGenerate}");\n        }\n\n        Debug.Log("Dataset generation complete!");\n    }\n\n    void RandomizeScene()\n    {\n        // Random lighting\n        if (sunLight != null)\n        {\n            sunLight.intensity = Random.Range(0.5f, 2.0f);\n            sunLight.transform.rotation = Quaternion.Euler(\n                Random.Range(30f, 80f),\n                Random.Range(0f, 360f),\n                0\n            );\n        }\n\n        // Random skybox\n        if (skyboxMaterials.Length > 0)\n        {\n            RenderSettings.skybox =\n                skyboxMaterials[Random.Range(0, skyboxMaterials.Length)];\n        }\n\n        // Spawn random objects\n        ClearSpawnedObjects();\n        int numObjects = Random.Range(5, 15);\n\n        for (int i = 0; i < numObjects; i++)\n        {\n            GameObject prefab = objectPrefabs[Random.Range(0, objectPrefabs.Length)];\n            Vector3 position = new Vector3(\n                Random.Range(-5f, 5f),\n                Random.Range(0.5f, 2f),\n                Random.Range(2f, 10f)\n            );\n\n            GameObject obj = Instantiate(prefab, position, Random.rotation);\n            obj.tag = "SpawnedObject";\n        }\n    }\n\n    void CaptureFrame(int index)\n    {\n        // Capture image\n        string imagePath = Path.Combine(\n            datasetPath, "images", $"img_{index:D4}.jpg"\n        );\n\n        ScreenCapture.CaptureScreenshot(imagePath);\n\n        // Generate YOLO labels\n        string labelPath = Path.Combine(\n            datasetPath, "labels", $"img_{index:D4}.txt"\n        );\n\n        GenerateYOLOLabels(labelPath);\n    }\n\n    void GenerateYOLOLabels(string filePath)\n    {\n        List<string> labels = new List<string>();\n\n        GameObject[] objects = GameObject.FindGameObjectsWithTag("SpawnedObject");\n\n        foreach (GameObject obj in objects)\n        {\n            // Get bounding box in camera view\n            Bounds bounds = GetObjectBounds(obj);\n\n            if (IsInCameraView(bounds))\n            {\n                // Calculate YOLO format (normalized coords)\n                Vector2 center = GetNormalizedCenter(bounds);\n                Vector2 size = GetNormalizedSize(bounds);\n\n                // Class ID (0 for now, extend for multiple classes)\n                int classId = 0;\n\n                labels.Add($"{classId} {center.x} {center.y} {size.x} {size.y}");\n            }\n        }\n\n        File.WriteAllLines(filePath, labels);\n    }\n\n    Bounds GetObjectBounds(GameObject obj)\n    {\n        Renderer renderer = obj.GetComponent<Renderer>();\n        return renderer != null ? renderer.bounds : new Bounds();\n    }\n\n    bool IsInCameraView(Bounds bounds)\n    {\n        Vector3 screenPoint = captureCamera.WorldToViewportPoint(bounds.center);\n        return screenPoint.z > 0 &&\n               screenPoint.x > 0 && screenPoint.x < 1 &&\n               screenPoint.y > 0 && screenPoint.y < 1;\n    }\n\n    Vector2 GetNormalizedCenter(Bounds bounds)\n    {\n        Vector3 screenPoint = captureCamera.WorldToViewportPoint(bounds.center);\n        return new Vector2(screenPoint.x, screenPoint.y);\n    }\n\n    Vector2 GetNormalizedSize(Bounds bounds)\n    {\n        // Simplified - calculate actual bounding box size\n        return new Vector2(0.1f, 0.1f); // Placeholder\n    }\n\n    void ClearSpawnedObjects()\n    {\n        GameObject[] objects = GameObject.FindGameObjectsWithTag("SpawnedObject");\n        foreach (GameObject obj in objects)\n        {\n            Destroy(obj);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"-lab-exercise-3-ml-dataset-creation",children:"\ud83d\udd2c Lab Exercise 3: ML Dataset Creation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Build a Unity scene for synthetic data generation"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create environment with varied lighting"}),"\n",(0,r.jsx)(n.li,{children:"Spawn 10-20 random objects per scene"}),"\n",(0,r.jsx)(n.li,{children:"Capture RGB + depth images"}),"\n",(0,r.jsx)(n.li,{children:"Generate YOLO-format labels"}),"\n",(0,r.jsx)(n.li,{children:"Export 500+ labeled images"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dataset Requirements:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"/dataset\n  /images\n    img_0000.jpg\n    img_0001.jpg\n    ...\n  /labels (YOLO format)\n    img_0000.txt\n    img_0001.txt\n    ...\n  /depth\n    depth_0000.png\n    ...\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deliverable:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Unity project with randomization"}),"\n",(0,r.jsx)(n.li,{children:"Python script to verify dataset"}),"\n",(0,r.jsx)(n.li,{children:"Train simple YOLOv8 model on your data"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-week-3-assessment-checklist",children:"\u2705 Week 3 Assessment Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up Unity-ROS 2 TCP connection"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Publish camera images from Unity to ROS"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Subscribe to velocity commands in Unity"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Build photorealistic environment"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Generate 100+ synthetic labeled images"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-module-summary",children:"\ud83d\udcdd Module Summary"}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-mastered",children:"Key Concepts Mastered"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Physics Simulation (Gazebo)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rigid body dynamics and collision detection"}),"\n",(0,r.jsx)(n.li,{children:"Force/torque application and constraints"}),"\n",(0,r.jsx)(n.li,{children:"World building with SDF format"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sensor Modeling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"LiDAR raytracing and obstacle detection"}),"\n",(0,r.jsx)(n.li,{children:"Camera rendering with distortion models"}),"\n",(0,r.jsx)(n.li,{children:"IMU sensor fusion (complementary filter)"}),"\n",(0,r.jsx)(n.li,{children:"Realistic noise and bias models"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Photorealistic Rendering (Unity)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Unity-ROS 2 bridge integration"}),"\n",(0,r.jsx)(n.li,{children:"High-fidelity visual environments"}),"\n",(0,r.jsx)(n.li,{children:"Synthetic dataset generation for ML"}),"\n",(0,r.jsx)(n.li,{children:"Camera image streaming to ROS"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autonomous Vehicles:"})," Test perception in simulated cities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Warehouse Robots:"})," Validate navigation before deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ML Training:"})," Generate infinite labeled training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human-Robot Interaction:"})," Prototype interfaces in VR"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-pitfalls--solutions",children:"Common Pitfalls & Solutions"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Problem"}),(0,r.jsx)(n.th,{children:"Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Robot falls through ground"}),(0,r.jsx)(n.td,{children:"Verify collision geometry exists"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sensors return no data"}),(0,r.jsx)(n.td,{children:"Check plugin is loaded and topics match"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Simulation too slow"}),(0,r.jsx)(n.td,{children:"Reduce physics step size or sensor rate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Unity-ROS disconnects"}),(0,r.jsx)(n.td,{children:"Check firewall, ensure IP/port correct"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Noisy sensor data"}),(0,r.jsx)(n.td,{children:"Tune Gaussian noise parameters"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"simulation-best-practices",children:"Simulation Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Start Simple:"})," Test with basic worlds before complex ones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validate Physics:"})," Compare sim vs real-world measurements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Profile Performance:"})," Monitor real-time factor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Version Control:"})," Track world files and robot models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Document Assumptions:"})," Note what's simplified"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Module 3: NVIDIA Isaac Sim (GPU-accelerated, photorealistic physics)\n\u2705 Implement SLAM with simulated sensor data\n\u2705 Train perception models on synthetic datasets\n\u2705 Deploy algorithms on real hardware"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-final-project-multi-environment-testing-suite",children:"\ud83c\udf93 Final Project: Multi-Environment Testing Suite"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective:"})," Test the same navigation algorithm in 3 different environments."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo World 1:"})," Indoor office with narrow hallways"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo World 2:"})," Outdoor terrain with slopes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Scene:"})," Warehouse with dynamic lighting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Navigation Controller:"})," Single ROS 2 node works in all 3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Report:"})," Compare success rates, timing"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Metrics to Measure:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Navigation success rate"}),"\n",(0,r.jsx)(n.li,{children:"Average completion time"}),"\n",(0,r.jsx)(n.li,{children:"Collision frequency"}),"\n",(0,r.jsx)(n.li,{children:"CPU/GPU usage"}),"\n",(0,r.jsx)(n.li,{children:"Sensor data quality"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Grading Rubric:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Gazebo worlds design quality (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Unity scene realism (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Controller robustness (30%)"}),"\n",(0,r.jsx)(n.li,{children:"Performance analysis report (20%)"}),"\n",(0,r.jsx)(n.li,{children:"Demo video showing all 3 envs (10%)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Submission:"})," GitHub repo + analysis PDF + 5-min video"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Official Documentation:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://gazebosim.org/tutorials",children:"Gazebo Tutorials"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/Unity-Technologies/Unity-Robotics-Hub",children:"Unity Robotics Hub"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/Unity-Technologies/ROS-TCP-Connector",children:"ROS-TCP-Connector"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://sdformat.org/",children:"SDF Format Spec"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Video Tutorials:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://youtube.com/gazebosim",children:"Gazebo Simulation Basics"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://learn.unity.com/course/unity-for-robotics",children:"Unity for Robotics Course"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://youtube.com/gazeboplugins",children:"Sensor Plugins Tutorial"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Community & Forums:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://answers.gazebosim.org/",children:"Gazebo Answers"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://forum.unity.com/forums/robotics.623/",children:"Unity Robotics Forum"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://discourse.ros.org/c/simulation",children:"ROS Discourse - Simulation"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Recommended Tools:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Blender:"})," Create custom 3D models for simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MeshLab:"})," Process and optimize mesh files"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CloudCompare:"})," Visualize and edit point clouds"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Books:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Gazebo Simulation" by Enis Bilgin'}),"\n",(0,r.jsx)(n.li,{children:'"Unity Game Development Cookbook" by Paris Buttfield-Addison'}),"\n",(0,r.jsx)(n.li,{children:'"Learning ROS for Robotics Programming" by Enrique Fernandez'}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);